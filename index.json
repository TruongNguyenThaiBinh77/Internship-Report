[{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Event Report \u0026ldquo;Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\u0026rdquo; Topic (Track 1): GenAI \u0026amp; Data (Generative AI \u0026amp; Data)\nI. EVENT OBJECTIVES The purpose of attending this event was to update on the latest trends and solutions on AWS, specifically focusing on 4 main objectives:\nComprehensive GenAI Strategy: Understanding how to build AI-driven software development lifecycle (AI-DLC) and the latest GenAI strategies Enterprise Security: Deep understanding of security in GenAI and AI Agents to ensure data safety Data Foundation: Learning how to build a Unified Data Foundation optimized for Analytics and AI Expert Connection: Direct interaction with engineers and experts from AWS Speakers:\nAWS Team: Jun Kai Loke (AI/ML Specialist), Kien Nguyen, Tamelly Lim, Binh Tran, Taiki Dang (Solutions Architects), Michael Armentano (Principal WW GTM Specialist).\nII. KEY TECHNICAL INSIGHTS 1. Unified Data Platform For AI to operate effectively, data needs to be processed seamlessly. AWS emphasizes the Zero-ETL model and breaking down data \u0026ldquo;silos\u0026rdquo;:\nEnd-to-End Process: From Ingestion $\\rightarrow$ Storage $\\rightarrow$ Processing $\\rightarrow$ Access $\\rightarrow$ Governance Service Ecosystem: Tight integration between Amazon S3, Glue, Redshift, Lake Formation, and OpenSearch Self-service Mindset: Empowering project teams to extract data autonomously while ensuring compliance with common standards 2. GenAI Strategy \u0026amp; Amazon Bedrock Amazon Bedrock: Plays a central role in selecting Foundation Models, implementing RAG (Retrieval-Augmented Generation), and optimizing cost/latency AgentCore \u0026amp; Amazon Nova: Strongly supports modern Agent frameworks like CrewAI, LangGraph, LlamaIndex, helping build complex automation tasks 3. Multi-layer Security for GenAI (Securing GenAI) Security is not only at the infrastructure level but must be applied following the \u0026ldquo;Defense in Depth\u0026rdquo; model:\n3 Security Layers: Infrastructure $\\rightarrow$ Model $\\rightarrow$ Application 5 Main Pillars: Compliance, Privacy, Controls, Risk Management, and Resilience Practical Tools: Using Bedrock Guardrails to review input/output content and OpenTelemetry for Observability 4. AI-Driven Development Lifecycle (AI-DLC) This is an important shift in software development thinking:\nEvolution: Shifting from AI-Assisted (AI supports code) $\\rightarrow$ AI-Driven (AI leads) $\\rightarrow$ AI-Managed (AI manages operations) Implementation: Integrating AI into all stages: from IaC (Infrastructure as Code), Automated Testing to risk management 5. Amazon SageMaker ‚Äì Unified Studio Unified environment for Data, Analytics, and AI, supporting Lakehouse architecture Comprehensive MLOps integration (Pipelines, Registry, Monitoring) helps accelerate bringing GenAI applications to market III. LESSONS LEARNED \u0026amp; KEY INSIGHTS From the above content, I have drawn 3 core lessons for project development:\nDesign Mindset: Need to build Data \u0026amp; AI systems with an \u0026ldquo;End-to-End\u0026rdquo; mindset from the start, avoiding fragmentation Governance principles must go hand in hand with Self-service capability Technical Architecture: Maximize Zero-ETL (integrate S3 $\\leftrightarrow$ Redshift/Aurora/DynamoDB) to minimize data pipeline operational effort Use AI Agents to automate processes instead of just using AI as a regular chatbot tool Reliability and Accuracy: To reduce AI Hallucination, must combine: Prompt Engineering + RAG + Fine-tuning Standard RAG process: $Input \\rightarrow Embedding \\rightarrow Context \\rightarrow LLM \\rightarrow Output$ IV. ACTION PLAN FOR WORK APPLICATION Based on the knowledge learned, I propose specific application directions:\n1. For current projects Features: Experiment with integrating AI Agents into Customer Support processes and Registration/Login support Safety: Apply Bedrock Guardrails and validation layers to ensure AI does not respond with sensitive or incorrect content 2. For development processes (Team \u0026amp; Learning) AI-DLC Model: Clearly divide tasks: AI is responsible for generating source code and writing documentation; humans focus on Review and Approve Infrastructure: Carefully consider using Serverless (AWS Lambda) for short-term tasks versus Container (ECS/Fargate) for long-term/complex tasks 3. Personal direction (Intern/Developer role) Business-First: Always ask \u0026ldquo;What is the business value?\u0026rdquo; before writing code or gathering requirements. Technology must serve business objectives Data Mindset: Recognize that a clean and structured data platform is a prerequisite for GenAI to operate effectively V. EXPERIENCE \u0026amp; EVALUATION The biggest highlight of the event was the workshop \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo;. This was a valuable opportunity to practice modernizing databases and applications.\nProfessionally: I clearly understood how to design a complete data pipeline and how to balance AI power with human control (Human-in-the-loop) Tools: Had practical exposure to Amazon Bedrock, AgentCore, and SageMaker Unified Studio Connections: Interacting with AWS experts helped me broaden my perspective on real-world problems (Use cases) and how to solve problems at enterprise scale Some images from the event Add your images here Summary: GenAI is not just a tool, but requires a comprehensive strategy from Data, Security to Architecture. Applying AI Agents and the AI-DLC model will be the key to improving productivity and changing how systems are operated in the near future.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tr∆∞∆°ng Nguy·ªÖn Th√°i B√¨nh\nPhone Number: 0869371050\nEmail: binhtntse182370@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh City\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1: Identity Management and Network Architecture (08/09 ‚Äì 12/09) Objective: Set up enterprise-standard AWS account and build secure Virtual Private Cloud (VPC).\nTasks completed this week: Day Date Task Activities 1 08/09 Account Setup \u0026amp; Cost Management - Create AWS Account with Root User security (MFA enabled)\n- Set up AWS Budgets for monthly cost and usage alerts\n- Study AWS Support plans (Basic, Developer, Business) 2 09/09 IAM - Part 1 - Apply Principle of Least Privilege\n- Create IAM Users and Groups (Admins, Developers, Auditors)\n- Configure Account Password Policy (CIS Benchmark compliant) 3 10/09 IAM - Part 2 - Create IAM Roles and Instance Profiles for EC2\n- Write custom JSON policies for granular access control\n- Practice EC2 accessing S3 via Instance Metadata Service 4 11/09 VPC Architecture - Theory \u0026amp; Design - Study VPC vs Default VPC concepts\n- Plan IP address space (CIDR 10.0.0.0/16)\n- Design Multi-AZ model with Public/Private Subnets 5 12/09 VPC Deployment - Create VPC, Subnets, Internet Gateway\n- Configure Route Tables for Public/Private subnets\n- Deploy NAT Gateway for Private subnet internet access Week 1 Achievements: Understood foundational AWS service groups:\nCompute (Amazon EC2) Networking (Amazon VPC) Identity \u0026amp; Access Management (IAM) Billing \u0026amp; Cost Management (AWS Budgets) Successfully created and secured AWS Account with MFA on Root User\nMastered IAM concepts:\nCreating Users, Groups with Managed Policies Writing custom JSON policies for granular access IAM Roles for EC2 (Instance Profiles) Designed and deployed enterprise VPC:\nMulti-AZ architecture for High Availability Public Subnets for Load Balancers, Bastion Hosts Private Subnets for Databases, Application Servers NAT Gateway for secure outbound internet access Applied FinOps thinking with AWS Budgets from day one\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This is a detailed technical report and worklog for the \u0026ldquo;First Cloud Journey\u0026rdquo; program. Designed as a 3-month capability development roadmap, from September 8, 2025 to December 7, 2025, this report simulates the transformation of a system architect from basic cloud concepts to deploying modern, complex architectures on Amazon Web Services (AWS).\nFrom Week 8 onwards, alongside AWS learning, I joined the Voltgo project - an electric vehicle rental platform. As a Frontend Developer, I was responsible for building main interfaces: User, Login, Booking, Blog, Vehicle, Station.\n13-Week Learning \u0026amp; Working Roadmap: Week 1: Identity Management and Network Architecture (08/09 ‚Äì 12/09)\nSet up enterprise-standard AWS account and build secure VPC Week 2: Basic Computing and Storage (15/09 ‚Äì 19/09)\nDeploy EC2, understand storage types (EBS, S3) and Lightsail Week 3: Databases and Managed Services (22/09 ‚Äì 26/09)\nTransition from self-managed to Managed Databases (RDS, DynamoDB, ElastiCache) Week 4: Scalability, Monitoring and CDN (29/09 ‚Äì 03/10)\nAuto Scaling, CloudWatch monitoring, Route 53 and CloudFront Week 5: Operations and Infrastructure as Code (06/10 ‚Äì 10/10)\nSystems Manager, CloudFormation, AWS CDK Week 6: Multi-layer Security Architecture (13/10 ‚Äì 17/10)\nWAF, KMS, Secrets Manager, GuardDuty, Cognito Week 7: Migration Strategy and Disaster Recovery (20/10 ‚Äì 24/10)\nVM Import/Export, DMS, SCT, Elastic Disaster Recovery, AWS Backup Week 8: AWS Cost Optimization \u0026amp; Voltgo Project Kickoff (27/10 ‚Äì 31/10)\nCost Explorer, Compute Optimizer, Transit Gateway + Start Voltgo Frontend project Week 9: AWS Containers \u0026amp; Voltgo Login/User Modules (03/11 ‚Äì 07/11)\nDocker, ECS, EKS + Build Login page, Register page, User management Week 10: AWS Serverless \u0026amp; Voltgo Vehicle/Station Modules (10/11 ‚Äì 14/11)\nLambda, API Gateway, Step Functions + Build Vehicle list, Station list with Map Week 11: AWS App Modernization \u0026amp; Voltgo Booking/Blog Modules (17/11 ‚Äì 21/11)\nMicroservices, CI/CD + Build Booking flow, Blog pages Week 12: AWS Data Analytics \u0026amp; Voltgo Deployment (24/11 ‚Äì 28/11)\nData Lake, Glue, Athena, SageMaker + Testing, CI/CD, Production deployment Week 13: Program Summary \u0026amp; Completion (01/12 ‚Äì 07/12)\nAWS Review, Security Audit, Certification Prep + Voltgo Final Testing \u0026amp; Handover "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Introduction to EV Rental AI Agent What is an AI Agent? An AI Agent is an intelligent system that can:\nUnderstand natural language queries Automatically select and execute appropriate tools/functions Make decisions based on context Provide structured responses with data Unlike traditional chatbots with fixed responses, AI Agents can reason and take actions dynamically.\nSystem Architecture Our EV Rental AI Agent uses a multi-layered architecture:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ User Interface ‚îÇ ‚Üê React Frontend (Chat UI) ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ HTTP/REST ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ FastAPI Server ‚îÇ ‚Üê Backend Orchestrator ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Strands ‚îÇ ‚îÇ PostgreSQL ‚îÇ ‚îÇ Agent SDK‚îÇ ‚îÇ (History) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí AWS Bedrock (Claude 3.5 Sonnet) ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Knowledge Base (Policies/FAQ) ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Backend API (Vehicles/Stations) Key Components Component Technology Purpose AI Model AWS Bedrock - Claude 3.5 Sonnet Natural language understanding \u0026amp; generation Agent Framework Strands Agent SDK Automatic tool selection \u0026amp; orchestration Backend API FastAPI (Python) REST API server for agent logic Database PostgreSQL Store chat history \u0026amp; sessions Frontend React + Chakra UI Interactive chat interface Knowledge Base AWS Bedrock KB Document retrieval (policies, FAQ) Core Features 1. Knowledge Base Search Agent searches through uploaded documents to answer questions about:\nRental policies Pricing information Booking procedures Terms and conditions Example Query:\n\u0026ldquo;Ch√≠nh s√°ch thu√™ xe c·ªßa b·∫°n l√† g√¨?\u0026rdquo;\nAgent Response:\n## üìã VinFast Rental Policies ### üìÑ Required Documents: - ‚úÖ Valid ID/Passport - ‚úÖ Driver\u0026#39;s License (Class B1+) - ‚úÖ Proof of Residence ### üí∞ Pricing: - **VF8**: 1,500,000 VNƒê/day - **VF9**: 2,000,000 VNƒê/day - **Deposit**: 10,000,000 VNƒê 2. Vehicle Search Agent queries the backend API to find available vehicles based on:\nLocation (city) Date range Vehicle model/type Response Format: Interactive vehicle cards with specs\n3. Charging Station Finder Agent retrieves nearby charging stations with:\nAddress and status Available chargers Distance (if location provided) Response Format: Station cards with real-time availability\nWorkshop Objectives By the end of this workshop, you will be able to:\n‚úÖ Configure AWS Bedrock - Enable Claude models and create a Knowledge Base ‚úÖ Build an AI Agent Backend - Use Strands SDK to orchestrate multiple tools ‚úÖ Deploy a Chat Interface - Create a responsive React chat UI ‚úÖ Test End-to-End - Interact with the AI agent and verify all functionalities Technology Stack AWS Services:\nAWS Bedrock (Claude 3.5 Sonnet v2) AWS Bedrock Knowledge Bases AWS S3 (for document storage) IAM (for access management) Backend:\nPython 3.11+ FastAPI Strands Agent SDK PostgreSQL SQLAlchemy Frontend:\nReact 18 Chakra UI Axios React Markdown Workshop Flow Step 1: Prerequisites ‚Üì Step 2: Setup AWS Bedrock \u0026amp; Knowledge Base ‚Üì Step 3: Deploy Backend API (FastAPI) ‚Üì Step 4: Deploy Frontend (React) ‚Üì Step 5: Test the AI Agent ‚Üì Step 6: Cleanup Resources Next: Let\u0026rsquo;s move to Prerequisites to prepare your environment.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Event Report \u0026ldquo;AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI\u0026rdquo; I. EVENT OVERVIEW Event Name: AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI\nObjectives:\nMaster Prompt Engineering techniques and available AI services on AWS Deep dive into RAG (Retrieval-Augmented Generation) and its enterprise applications Update on Agentic AI trends and solutions for moving AI Agents from Proof of Concept (POC) to Production Explore real-time Voice AI technology with Pipecat Speaker List:\nLam Tuan Kiet - Sr DevOps Engineer (FPT Software) Danh Hoang Hieu Nghi - AI Engineer (Renova Cloud) Dinh Le Hoang Anh - Cloud Engineer Trainee (First Cloud AI Journey) II. KEY KNOWLEDGE INSIGHTS Through presentations from speakers at FPT Software, Renova Cloud, and First Cloud AI Journey, I have compiled the following core knowledge groups:\n1. Prompt Engineering \u0026amp; Foundation Models (Core Foundation) I consolidated my understanding of how to communicate with Foundation Models on Amazon Bedrock. The highlight is the effective difference between:\nZero-shot / Few-shot Prompting: Techniques to guide the model through direct instructions or providing sample examples Chain of Thought (CoT): Technique requiring the model to \u0026ldquo;reason step by step\u0026rdquo;, significantly increasing accuracy when handling complex logic problems 2. AWS AI Pretrained Services The event systematized \u0026ldquo;Ready-to-use\u0026rdquo; APIs that help integrate artificial intelligence without retraining models from scratch:\nImage/Video: Amazon Rekognition Language \u0026amp; Text: Amazon Translate, Comprehend, Textract (OCR) Audio: Amazon Polly (Text-to-Speech) and Transcribe (Speech-to-Text) 3. RAG - Retrieval Augmented Generation This is an important solution to address the \u0026ldquo;hallucinations\u0026rdquo; problem of AI when applied to enterprises. The process is clarified through:\nUsing Amazon Titan Text Embeddings V2 to vectorize data for semantic search Leveraging Knowledge Bases for Amazon Bedrock to manage the entire process from Chunking, Vector storage, to Retrieval and answer Generation 4. Shift to Agentic AI \u0026amp; Production Challenges I grasped the evolution from GenAI Assistants (rule-following) to GenAI Agents (goal-oriented, autonomous). However, the transition from POC to Production faces major obstacles:\nPerformance and Scalability Security, Governance, and Access Control Complexity in memory and context management 5. Amazon Bedrock AgentCore Solution To address the above challenges, AWS introduces AgentCore with components:\nRuntime \u0026amp; Memory: Execution environment and ability to remember interaction history Identity \u0026amp; Gateway: Identity management and security Code Interpreter: Allows Agents to write and run code to process complex data Observability: Tools to monitor and audit Agent behavior 6. Pipecat Framework (Voice AI) An impressive open-source framework for multimodal virtual assistants, operating through real-time pipeline mechanism: $WebRTC \\rightarrow STT \\rightarrow LLM \\rightarrow TTS \\rightarrow Output$.\nIII. EVALUATION \u0026amp; LESSONS LEARNED After attending the workshop, I drew 3 important lessons and mindset changes:\n1. Mindset shift from \u0026ldquo;Q\u0026amp;A\u0026rdquo; to \u0026ldquo;Action\u0026rdquo; (Agentic AI) Previously, I often limited AI to chatting or summarizing. However, the Agentic AI concept opened up a vision of true \u0026ldquo;virtual employees\u0026rdquo;. The Agent\u0026rsquo;s ability to plan and use tools is a major step forward helping automate complex processes without continuous human intervention.\n2. Solving the \u0026ldquo;Production\u0026rdquo; problem with AgentCore I deeply appreciated the discussion about the \u0026ldquo;Deep gap\u0026rdquo; between POC and Production. Using tools like Amazon Bedrock AgentCore is not just a technical issue but the key to building enterprise trust. Security and control layers (Observability) are mandatory for enterprises to dare entrust work to AI.\n3. Real-world application potential of Pipecat The combination of WebRTC and AI Model in Pipecat creates an extremely low-latency conversation experience. This opens up many practical application ideas for me such as: intelligent customer care hotline, recruitment interview assistant, or real-time language learning applications.\nIV. CONCLUSION Workshop \u0026ldquo;Generative AI \u0026amp; Agentic AI on AWS\u0026rdquo; provided a comprehensive view and clear technology roadmap:\nPresent: Focus on mastering RAG and Prompt Engineering Future: Towards the era of Agentic AI and autonomous Agents Tools: Leverage the AWS ecosystem (Bedrock, AgentCore) and Frameworks like Pipecat to realize breakthrough ideas The workshop not only provided theoretical knowledge but also equipped me with practical vision on deploying AI solutions in enterprise environments, especially the transition from experimental projects to sustainable and reliable production systems.\nSome images from the event Add your images here Overall, the event helped me expand my understanding of Generative AI and Agentic AI trends, while providing specific tools and methodologies to apply in practice.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites for EV Rental AI Agent Workshop Before starting this workshop, ensure you have the following requirements ready:\n1. AWS Account You need an AWS Account with appropriate permissions to:\nAccess AWS Bedrock service Create and manage IAM users Create S3 buckets (for Knowledge Base) Create Knowledge Bases Note: Bedrock is available in specific regions. Recommended regions:\nus-west-2 (Oregon) us-east-1 (N. Virginia) ap-southeast-1 (Singapore) 2. IAM User with Bedrock Permissions You need to create an IAM User with AWS Bedrock access for your application.\nStep 1: Create IAM User\nGo to AWS Console ‚Üí IAM ‚Üí Users ‚Üí Create User User name: bedrock-agent-user ‚úÖ Check: Provide user access to the AWS Management Console (optional) ‚úÖ Select: I want to create an IAM user Click Next Step 2: Attach Permissions\nSelect: Attach policies directly Search and select these policies: ‚úÖ AmazonBedrockFullAccess - Full access to Bedrock models and Knowledge Bases ‚úÖ (Optional) AmazonS3ReadOnlyAccess - If using Knowledge Base with S3 Click Next ‚Üí Create User Step 3: Create Access Keys\nClick on the newly created user: bedrock-agent-user Go to Security credentials tab Scroll down to Access keys ‚Üí Click Create access key Select use case: Application running outside AWS Click Next ‚Üí Create access key ‚ö†Ô∏è IMPORTANT: Copy and save: Access Key ID (example: AKIAIOSFODNN7EXAMPLE) Secret Access Key (shown only once, example: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY) Click Done ‚ö†Ô∏è Security Note:\n# Save to .env file (DO NOT commit to Git) AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID_HERE AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY_HERE AWS_REGION=us-west-2 3. Development Environment 3.1. Python Environment Python 3.11 or higher Package manager: pip Verify installation:\npython --version # Expected: Python 3.11.x or higher pip --version 3.2. Node.js Environment Node.js 18+ and npm Required for React frontend Verify installation:\nnode --version # Expected: v18.x.x or higher npm --version 3.3. PostgreSQL Database PostgreSQL 14+ installed locally or use Docker Option 1: Install locally\nDownload from: https://www.postgresql.org/download/ Create database: ev_rental_db Option 2: Use Docker\ndocker run -d \\ --name postgres-ev \\ -e POSTGRES_PASSWORD=password \\ -e POSTGRES_DB=ev_rental_db \\ -p 5432:5432 \\ postgres:14 Verify PostgreSQL:\n# Check PostgreSQL is running psql --version # Connect to database psql -U postgres -d ev_rental_db 4. Code Editor \u0026amp; Tools VS Code or your preferred IDE Git for cloning repositories Postman or cURL for API testing (optional) Install VS Code:\nDownload from: https://code.visualstudio.com/ Install Git:\n# macOS brew install git # Windows # Download from: https://git-scm.com/download/win # Verify git --version 5. AWS CLI (Optional) Install AWS CLI to interact with AWS services from command line:\n# macOS/Linux curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / # Windows # Download from: https://awscli.amazonaws.com/AWSCLIV2.msi # Verify aws --version Configure AWS CLI:\naws configure # Enter your Access Key ID: AKIA5GPEMGJZK6E7PMEB # Enter your Secret Access Key: (paste your secret key) # Default region name: us-west-2 # Default output format: json Test AWS CLI:\n# List available Bedrock models aws bedrock list-foundation-models --region us-west-2 # Check your identity aws sts get-caller-identity Prerequisites Checklist Before proceeding to the next step, ensure you have:\n‚úÖ AWS Account with Bedrock access in supported region ‚úÖ IAM User created with AmazonBedrockFullAccess policy ‚úÖ Access Key ID and Secret Access Key saved securely ‚úÖ Python 3.11+ installed and verified ‚úÖ Node.js 18+ and npm installed and verified ‚úÖ PostgreSQL 14+ database running ‚úÖ Code editor (VS Code recommended) installed ‚úÖ Git installed and configured ‚úÖ (Optional) AWS CLI installed and configured Estimated Costs This workshop uses the following AWS services:\nService Estimated Cost Notes AWS Bedrock - Claude 3.5 Sonnet ~$0.50 - $2.00 Pay per API call (input/output tokens) AWS Bedrock - Knowledge Base ~$0.10 - $0.50 Vector storage + retrieval S3 Storage ~$0.02 Minimal for documents Data Transfer ~$0.05 Usually within free tier Total ~$0.67 - $2.57 For the entire workshop üí° Tip: Remember to clean up resources after the workshop to avoid ongoing charges!\nNext: Proceed to Setup AWS Bedrock to enable models and create Knowledge Base.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"üìÑ View Full Proposal Document (Google Docs)\nThe EV Station-based Rental System Electric Vehicle Rental and Return Software at Fixed Stations ‚Äì A Green Mobility Solution for Smart Cities 1. Executive Summary The EV Station-based Rental System is developed to provide an all-in-one platform for electric vehicle rental and charging management. It integrates real-time rental, payment, and charging station access through a unified cloud-native solution. The system features a React Native mobile app and a Spring Boot backend deployed on AWS ECS Fargate, with PostgreSQL (RDS) and Redis (ElastiCache) for data and caching. User authentication is managed via Amazon Cognito, and global delivery is optimized using CloudFront. Designed under the AWS Well-Architected Framework, the platform ensures scalability, high availability, and security while maintaining cost efficiency.\n2. Problem Statement What‚Äôs the Problem? Current electric vehicle (EV) rental services are fragmented, requiring users to switch between multiple apps to locate, book, and manage rentals at fixed points. This creates inconvenience, slow performance, and unreliable experiences ‚Äî users often arrive at ‚Äúunavailable‚Äù or ‚Äúoffline‚Äù rental points, leading to frustration and loss of trust.\nFor vehicle owners and operators, manual fleet management, booking coordination, and maintenance tracking result in operational inefficiencies and lost revenue. Currently, there is no unified, real-time platform connecting renters, vehicle owners, and rental point operators.\nThe Solution The EV Station-based Rental System consolidates EV rental and return at fixed points into a single, cloud-native platform. Built with React Native for mobile and Spring Boot for backend, the system delivers real-time booking, vehicle tracking, and payment integration.\nKey AWS services include ECS Fargate for compute, RDS PostgreSQL for data storage, ElastiCache for low-latency performance, API Gateway and Cognito for secure access, and CloudFront for global content delivery. The platform supports both fleet-based and peer-to-peer (P2P) vehicle registration, providing a centralized interface for users and operators to manage rentals efficiently, securely, and at scale.\nBenefits and Return on Investment The platform eliminates manual coordination and fragmented applications, offering a unified, automated experience for renters and fleet owners. Real-time data ensures reliability and transparency regarding vehicle availability and rental point status.\nDesigned under the AWS Well-Architected Framework, the system minimizes operational costs with a serverless, pay-per-use model while maintaining scalability and 99.99% uptime. Within 12‚Äì24 months, the platform is projected to reach 50,000+ monthly active users, onboard 200+ rental points, and deliver significant time, cost, and operational efficiencies for both users and operators.\n3. Solution Architecture The VoltGo platform adopts a serverless and fully private AWS architecture for secure and scalable backend operations. Backend run on Amazon ECS Fargate, connecting to Aurora PostgreSQL Serverless v2 for relational data and ElastiCache Serverless (Redis) for caching. All workloads are deployed in private subnets across multiple Availability Zones and accessed securely through API Gateway via AWS PrivateLink to an internal Network Load Balancer. User authentication is managed by Amazon Cognito, while the frontend is hosted on Amazon S3 and delivered globally via CloudFront, protected by AWS WAF and ACM SSL/TLS. Monitoring and secrets management are handled by CloudWatch and Secrets Manager, with the entire infrastructure provisioned through Terraform IaC. This architecture ensures high security, elasticity, and cost efficiency suitable for the current development stage and future production scaling.\nAWS Services Used Amazon ECS Fargate: Serverless container orchestration for backend microservices. Amazon Aurora PostgreSQL Serverless v2: Scalable, multi-AZ relational database. Amazon ElastiCache Serverless (Redis): In-memory caching for low-latency data access. Amazon API Gateway: Secure REST API entry point integrated via PrivateLink. Amazon Cognito: User authentication and authorization with JWT and MFA. Amazon CloudFront + S3: Global content delivery and static hosting with WAF protection. AWS Secrets Manager: Centralized secret storage and automatic rotation. Amazon CloudWatch: Unified monitoring, logging, and alerting for all services. AWS WAF + ACM: Edge-level security and SSL/TLS certificate management. Component Design Frontend:React/Vue.js web application hosted on Amazon S3 and delivered via CloudFront, secured with AWS WAF and ACM SSL/TLS certificates. API Layer: Amazon API Gateway provides the public API endpoint, connecting privately to backend services through AWS PrivateLink to an internal Network Load Balancer. Compute Layer: Amazon ECS Fargate runs containerized microservices across multiple Availability Zones, scaling automatically based on CPU and memory utilization. Database Layer:Amazon Aurora PostgreSQL Serverless v2 stores relational data with a writer and read replica for high availability and automated scaling. Caching Layer: Amazon ElastiCache Serverless (Redis) caches session and booking data to reduce database load and improve response time. Authentication: Amazon Cognito handles user registration, login, and JWT-based authorization with optional MFA support. Storage: Amazon S3 manages static assets and user uploads, accessible only through CloudFront via Origin Access Control (OAC). Monitoring \u0026amp; Security: Amazon CloudWatch tracks logs and performance metrics, while AWS Secrets Manager securely stores credentials with automatic rotation. 4. Technical Implementation Implementation Phases This project has two main parts‚Äîdeveloping the backend locally and deploying it to the AWS cloud‚Äîeach following four key phases:\n1.Build and Design Architecture: Develop and test backend services locally using Docker Compose, PostgreSQL, and Redis. Design the AWS serverless architecture including ECS Fargate, Aurora Serverless, ElastiCache, and API Gateway with PrivateLink connections. (Pre-deployment phase) 2.Estimate Cost and Validate Feasibility: Use AWS Pricing Calculator to estimate the monthly cost of ECS tasks, Aurora capacity units, and CloudFront bandwidth. Adjust design decisions to ensure cost-effectiveness and smooth migration. 3.Configure and Deploy Infrastructure: Build and deploy cloud infrastructure using Terraform for IaC. Configure VPC, ECS, Aurora, ElastiCache, Cognito, and CloudFront. Validate IAM roles, networking, and private-only access via VPC Endpoints. 4.Test, Optimize, and Release: Deploy Dockerized services to ECS Fargate, test API Gateway ‚Üí PrivateLink ‚Üí NLB ‚Üí ECS flow, and verify database connections. Enable CloudWatch monitoring, auto-scaling, and WAF protection. Optimize scaling thresholds and document final architecture. Technical Requirements\nBackend Services: Node.js or Spring Boot microservices for Auth, Booking, and Payment, containerized with Docker and deployed to ECS Fargate (2‚Äì10 tasks, auto-scaling). Database Layer: Amazon Aurora PostgreSQL Serverless v2 with writer and reader instances, supporting automatic scaling and multi-AZ high availability. Caching Layer: Amazon ElastiCache Serverless (Redis 7.1) for session caching and frequently accessed data. Authentication: Amazon Cognito manages user registration, JWT-based authentication, and optional MFA, integrated with API Gateway. Storage \u0026amp; Content Delivery: Frontend hosted on Amazon S3 and distributed via CloudFront, protected by AWS WAF and ACM SSL/TLS certificates. Secrets \u0026amp; Monitoring: AWS Secrets Manager for storing credentials (DB, Redis, JWT keys) with 30-day rotation. Amazon CloudWatch for logging, metrics, and scaling alarms. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1: Foundation \u0026amp; Design (Weeks 1-2) Week 1: Finalize MVP scope (P0 User Stories), define user flows, and approve the AWS architecture. Week 2: FE Lead finalizes UI/UX mockups. Backend provisions core AWS (VPC, S3, ECR, Aurora). Phase 2: Core MVP Development (Weeks 3-8) Weeks 3-4: Backend builds User Auth (Cognito) \u0026amp; core APIs (API Gateway, ECS). Weeks 5-6: All teams (FE/BE/Mobile) build core screens (Login, Search, Details) and the Booking Engine APIs. Weeks 7-8: Integration of KYC flow (Lambda, Textract, Rekognition) and Payment Gateway integration. Phase 3: Testing \u0026amp; UAT (Weeks 9-10) Week 9: Full End-to-End (E2E) testing. QA is performed by the 5-person dev team, as no dedicated QA is allocated. Week 10: Stakeholder User Acceptance Testing (UAT) and final critical bug fixing. Phase 4: Launch (Week 11) Week 11: Production deployment, Go-live, and intensive Hypercare monitoring via CloudWatch. 6. Budget Estimation This budget estimate is based on the provided AWS architecture diagram and the \u0026ldquo;cheapest possible\u0026rdquo; MVP launch strategy, maximizing Free Tier usage.\nInfrastructure Costs AWS Services (Monthly Estimate): Amazon Route 53: $0.50/month (1 hosted zone). AWS WAF: $6.00/month (1 WebACL + 1 Rule + minimal requests). AWS S3 Standard: $0.00/month (Stays within 5GB Always Free tier). Amazon CloudFront: $0.00/month (Stays within 1TB/10M request Always Free tier). AWS Cognito: $0.00/month (Stays within 10,000 MAU free tier). Amazon API Gateway: $0.00/month (Stays within 1M request 12-month free tier). AWS Lambda: $0.00/month (Stays within 1M request Always Free tier). Amazon Textract/Rekognition: $0.00/month (Stays within 12-month free tier for KYC). Application Load Balancer: $17.52/month (1 ALB, minimal processing). VPC Endpoint (PrivateLink): $7.30/month (1 Endpoint, 1 AZ, 1GB data). Amazon ECS on Fargate: ~$20.00/month (Assumes 2 minimal 24/7 containers, e.g., 0.25 vCPU/0.5GB RAM). Amazon Aurora Serverless v2: ~$25.00/month (Minimal ACUs, configured to scale to near-zero). Amazon ElastiCache Serverless: ~$10.00/month (Minimal usage). Amazon CloudWatch: $0.00/month (Stays within 5GB log Always Free tier). Amazon ECR: ~$0.10/month (Minimal storage over 500MB free tier). Total: ~$86.42/month, ~$1,037.04/12 months\n7. Risk Assessment Risk Matrix System Downtime: High impact, medium probability. Data Sync Errors (Between Stations \u0026amp; Server): Medium impact, high probability. OCR Verification Failure: Medium impact, medium probability. Vehicle Shortage or Low Battery at Stations: High impact, high probability. Operational Mistakes by Staff: Medium impact, medium probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies System: Use load-balanced cloud servers with auto-scaling and failover backup. Data Sync: Implement offline caching and periodic background synchronization. OCR Verification: Combine AI-based ID recognition with manual approval option. Vehicle Management: Real-time tracking of battery and vehicle status; predictive restocking via analytics. Staff Operations: Provide training and digital checklists to reduce human error. Cost: Set up cloud cost monitoring and optimization alerts. Contingency Plans Enable offline mode for station staff when Internet is unavailable. Activate backup servers in case of major downtime. Provide manual check-in/out workflow for rentals during system outages. Deploy mobile maintenance team to handle vehicle or battery issues at stations. Suspend or limit reservations dynamically if vehicle supply falls below safe threshold. 8. Expected Outcomes Technical Improvements: Real-time monitoring of all EV stations and rental status. Automated verification and e-contract signing replace manual paperwork. Centralized dashboard for admins to manage fleet, customers, and staff. System scalable to 20+ rental stations in the next deployment phase. Long-term Value Establishes a reliable EV mobility infrastructure for urban areas. Builds data foundation for future AI-powered demand forecasting. Enables integration with smart city and green transportation networks. Serves as a reusable platform for expanding to nationwide EV-sharing projects. Short to Medium-term Benefits Faster customer onboarding (from 15 mins ‚Üí \u0026lt;5 mins). Increased fleet utilization rate by 30% through data-driven scheduling. Improved accuracy of rental records and payment reconciliation. Enhanced user satisfaction via seamless booking and transparent billing. "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2: Basic Computing and Storage (15/09 ‚Äì 19/09) Objective: Deploy virtual servers (EC2), understand storage types (EBS, S3) and simplified compute models (Lightsail).\nTasks completed this week: Day Date Task Activities 1 15/09 Amazon EC2 - Launch \u0026amp; Connect - Analyze instance types: T3 (Burstable), C5 (Compute), R5 (Memory)\n- Launch Amazon Linux 2023 instance\n- SSH connection with Key Pairs (.pem/.ppk) 2 16/09 EBS Storage \u0026amp; Windows Workloads - Create and mount gp3 EBS volume to Linux instance\n- Deploy Windows Server 2022, connect via RDP\n- Create EBS Snapshots for backup 3 17/09 Cloud9 Development Environment - Set up browser-based IDE with AWS CLI, SAM, Docker pre-installed\n- Remote access without opening SSH port\n- Experience code editing and terminal directly from browser 4 18/09 Amazon S3 Basics - Create S3 Bucket and upload media files\n- Configure Static Website Hosting\n- Write Bucket Policy JSON for public GetObject access 5 19/09 S3 Advanced \u0026amp; Security - Analyze Storage Classes: Standard, Intelligent-Tiering, Glacier\n- Set up Lifecycle Policy to move old objects to Glacier\n- Enable Block Public Access, Versioning, Default Encryption (SSE-S3) Week 2 Achievements: EC2 Mastery:\nUnderstand instance type families and use cases Successfully launched and connected to both Linux and Windows instances Created EBS volumes and snapshots for data persistence Cloud Development:\nSet up Cloud9 IDE eliminating local environment issues Experienced managed access without public SSH ports Object Storage:\nDeployed S3 static website with proper access policies Implemented cost optimization with Lifecycle Policies Applied security best practices: Block Public Access, Versioning, Encryption Key Concepts:\nEBS is persistent storage independent of EC2 lifecycle S3 provides unlimited scalable object storage Incremental backup mechanism with Snapshots "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report \u0026ldquo;AWS Cloud Mastery Series #2: From DevOps, IaC to Containers \u0026amp; Observability\u0026rdquo; I. EVENT OVERVIEW Event Name: AWS Cloud Mastery Series #2: From DevOps, IaC to Containers \u0026amp; Observability\nObjectives:\nMindset Standardization: Deep understanding of Value Cycle and the core role of DevOps Infrastructure Modernization (IaC): Transition from manual operations (ClickOps) to Infrastructure as Code Application Optimization (Containerization): Master the strategy of choosing appropriate container platforms (App Runner, ECS, EKS) Comprehensive Observability: Build proactive monitoring systems with CloudWatch and X-Ray Speaker List:\nAWS Experts \u0026amp; Cloud Engineers Team: Sharing about system architecture, Platform Engineering strategy, and in-depth technical demos.\nII. KEY KNOWLEDGE INSIGHTS Through sharing from AWS experts and Cloud engineers, I have distilled the following foundational knowledge blocks:\n1. DevOps Mindset \u0026amp; CI/CD Pipeline The event began by redefining DevOps not just as tools, but as a culture of optimizing value flow.\nValue Cycle: A closed-loop 5-step process aimed at increasing delivery Speed while ensuring Stability.\nEffective Pipeline Strategy:\nClearly distinguish Concepts:\nContinuous Integration (CI): Fail fast, integrate code daily Continuous Delivery: Automate to Staging, requires human approval for Production Continuous Deployment: 100% automation to Production \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo; Principle: Source code is built only once into a Binary package (Artifact). Subsequent environments (Staging, Prod) reuse this same Artifact to ensure absolute consistency.\nFail Fast Conditions: Pipeline must stop immediately when encountering compilation errors, Code Style violations, security vulnerabilities, or slow tests.\n2. Infrastructure as Code (IaC) - From ClickOps to Code This section addresses the problem of eliminating manual operations (\u0026ldquo;ClickOps\u0026rdquo;) that are error-prone and difficult to scale. I analyzed 3 main tools in depth:\nAWS CloudFormation (Native):\nUses YAML/JSON Manages resources in Stack units Deleting a Stack means deleting all related resources Terraform (Multi-Cloud):\nUses HCL language Strength is multi-platform support Safe workflow: $Code \\rightarrow Plan \\text{ (Review changes)} \\rightarrow Apply$ AWS CDK (Cloud Development Kit):\nAllows defining infrastructure using programming languages (Python, TypeScript\u0026hellip;) Constructs: From L1 (Detailed configuration) to L3 (Complex architecture patterns) Drift Detection: Important feature to detect discrepancies between Code and Reality (due to someone manually editing on Console) 3. Containerization Strategy Choosing a Container platform depends on scale and needs:\nAmazon ECS: Simple, deeply integrated with AWS, suitable for teams wanting quick operations Amazon EKS: Based on standard Kubernetes, large ecosystem, suitable for Enterprise or complex/Hybrid systems AWS App Runner: \u0026ldquo;Zero-ops\u0026rdquo; solution for Web App/API, automatically from Source/Image to HTTPS URL without server configuration Compute Options:\nEC2 Launch Type: Highest control but costly operations (patching, scaling) AWS Fargate (Serverless): AWS handles infrastructure, users only need to define CPU/RAM for Tasks 4. Observability - Monitoring \u0026amp; Optimization Closing the development lifecycle with deep observation capability:\nAmazon CloudWatch: \u0026ldquo;Eyes and Ears\u0026rdquo; of the system (Metrics, Logs, Alarms) AWS X-Ray: Solves the \u0026ldquo;finding a needle in a haystack\u0026rdquo; problem in Microservices through Distributed Tracing (tracking requests through multiple services to find bottlenecks) III. EVALUATION \u0026amp; LESSONS LEARNED Participating in this workshop series significantly changed my awareness and skills:\n1. Shift from \u0026ldquo;Ops\u0026rdquo; to \u0026ldquo;Platform Engineering\u0026rdquo; I realized that the role of modern DevOps is not to chase after Developers to manually deploy code. DevOps is the architect of the \u0026ldquo;Highway\u0026rdquo; (Pipeline \u0026amp; Platform). A good Platform allows Developers to Self-service creating environments and deploying code quickly while staying within safe corridors (Governance) that the DevOps team establishes.\n2. Operational Discipline Lessons on Artifact Management and Drift Detection are golden rules. In Enterprise environments, consistency is vital. Building differently across environments or manual changes to systems managed by code must be strictly prohibited.\n3. Smart Tool Selection Strategy There is no \u0026ldquo;best\u0026rdquo; tool, only the \u0026ldquo;most appropriate\u0026rdquo; tool:\nNeed absolute stability and deepest support for new AWS services $\\rightarrow$ Choose CloudFormation Enterprise using Multi-cloud or Hybrid-cloud $\\rightarrow$ Terraform is optimal Developer team strong in programming, needs to build complex architecture quickly and reuse code $\\rightarrow$ AWS CDK Simple Web App $\\rightarrow$ Use App Runner instead of spending resources operating Kubernetes cluster IV. CONCLUSION Series \u0026ldquo;DevOps \u0026amp; IaC Mastery\u0026rdquo; provided a complete roadmap for the Cloud journey:\nMindset: Shift from manual work to automation and measurement by metrics Infrastructure: Master IaC for scalable, reproducible systems and drift control Operations: Combine flexible Containerization and deep Observability to ensure stable, high-performance systems This is a solid knowledge foundation for me to confidently build and operate large-scale software systems on AWS.\nSome images from the event Add your images here Overall, the workshop helped me gain a comprehensive view of modern DevOps, from mindset, IaC tools, containerization strategy to observability, creating a solid foundation for building and operating Cloud-native systems.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.3-setup-bedrock/","title":"Setup AWS Bedrock","tags":[],"description":"","content":"Setting up AWS Bedrock \u0026amp; Knowledge Base In this section, you will configure AWS Bedrock to use Claude 3.5 Sonnet and create a Knowledge Base for document retrieval.\nStep 1: Enable Model Access IMPORTANT: You must enable model access before using Bedrock, otherwise you\u0026rsquo;ll get ValidationException errors.\nGo to AWS Console ‚Üí Services ‚Üí Bedrock In the left sidebar, click Model access (under Foundation models) Click Manage model access button (orange) Find and select these models: ‚úÖ Anthropic - Claude 3.5 Sonnet v2 (anthropic.claude-3-5-sonnet-20241022-v2:0) ‚úÖ Amazon - Titan Embeddings G1 - Text (for Knowledge Base) Click Request model access (bottom right) Wait for approval: Instant access models: Available immediately (green ‚úÖ) Other models: Wait 5-30 minutes (status changes from \u0026ldquo;In progress\u0026rdquo; ‚Üí \u0026ldquo;Access granted\u0026rdquo;) Verify models are enabled:\n# Using AWS CLI aws bedrock list-foundation-models --region us-west-2 # Or check in Console: # Bedrock ‚Üí Model access ‚Üí Status must be \u0026#34;Access granted\u0026#34; Step 2: Create S3 Bucket for Knowledge Base Knowledge Base requires an S3 bucket to store documents.\nGo to S3 ‚Üí Create bucket Bucket name: ev-rental-knowledge-docs (must be globally unique) Region: Same as your Bedrock region (e.g., us-west-2) Block all public access: ‚úÖ Enabled (recommended) Click Create bucket Step 3: Upload Documents to S3 Upload your rental policy documents (PDF, TXT, DOCX):\nSample documents to upload:\nrental-policy.pdf - Rental policies and terms pricing.pdf - Vehicle pricing information faq.txt - Frequently asked questions booking-process.pdf - How to book a vehicle Upload via Console:\nGo to your S3 bucket: ev-rental-knowledge-docs Click Upload ‚Üí Add files Select your documents Click Upload Upload via AWS CLI:\naws s3 cp rental-policy.pdf s3://ev-rental-knowledge-docs/ aws s3 cp pricing.pdf s3://ev-rental-knowledge-docs/ aws s3 cp faq.txt s3://ev-rental-knowledge-docs/ aws s3 cp booking-process.pdf s3://ev-rental-knowledge-docs/ Step 4: Create Knowledge Base Go to Bedrock ‚Üí Knowledge Bases ‚Üí Create Knowledge base name: ev-rental-knowledge-base Description: \u0026ldquo;VinFast EV rental policies and FAQs\u0026rdquo; Click Next Data source configuration:\nData source name: rental-docs S3 URI: s3://ev-rental-knowledge-docs/ Click Next Embeddings model:\nSelect: Titan Embeddings G1 - Text (amazon.titan-embed-text-v1) Vector database: Choose Bedrock managed (OpenSearch Serverless) (easiest option) Click Next Review and create:\nReview all settings Click Create knowledge base Wait for creation to complete (2-3 minutes) Step 5: Sync Data Source After Knowledge Base is created, you need to sync the data:\nIn your Knowledge Base, go to Data sources tab Select your data source: rental-docs Click Sync button Wait for sync to complete (check status: \u0026ldquo;Syncing\u0026rdquo; ‚Üí \u0026ldquo;Ready\u0026rdquo;) This process indexes all documents and creates vector embeddings Sync status:\nüîÑ Syncing: In progress ‚úÖ Ready: Completed successfully ‚ùå Failed: Check S3 permissions or document formats Step 6: Get Knowledge Base ID You\u0026rsquo;ll need this ID for your backend application:\nIn your Knowledge Base page Copy the Knowledge Base ID (format: 89CI1JSSE4 or similar) Save it in your notes - you\u0026rsquo;ll use it in the next step Example Knowledge Base ID:\nKnowledge Base ID: 89CI1JSSE4 Knowledge Base ARN: arn:aws:bedrock:us-west-2:123456789:knowledge-base/89CI1JSSE4 Step 7: Test Knowledge Base (Optional) Test your Knowledge Base directly in the console:\nGo to your Knowledge Base Click Test tab Enter a question: \u0026ldquo;What is the rental policy?\u0026rdquo; Click Run Verify it returns relevant information from your documents Verification Checklist Before moving to the next step, ensure:\n‚úÖ Claude 3.5 Sonnet v2 model access is granted ‚úÖ Titan Embeddings model access is granted ‚úÖ S3 bucket created with documents uploaded ‚úÖ Knowledge Base created and data synced successfully ‚úÖ Knowledge Base ID saved ‚úÖ Test query returns relevant results Troubleshooting Issue: \u0026ldquo;ValidationException: Model not enabled\u0026rdquo;\nSolution: Go to Bedrock ‚Üí Model access and enable the model Issue: \u0026ldquo;Sync failed\u0026rdquo;\nCheck S3 bucket permissions Verify document formats (PDF, TXT, DOCX supported) Check CloudWatch Logs for detailed errors Issue: \u0026ldquo;No results from Knowledge Base\u0026rdquo;\nEnsure documents are uploaded to S3 Run sync again Wait a few minutes after sync completes Try different query phrasing Next: Proceed to Deploy Backend API to build the FastAPI server.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3: Databases and Managed Services (22/09 ‚Äì 26/09) Objective: Transition from self-managed databases on EC2 to Managed Database services (RDS, DynamoDB) to reduce operational overhead.\nTasks completed this week: Day Date Task Activities 1 22/09 Amazon RDS Deployment - Deploy RDS with MySQL/PostgreSQL engine\n- Enable Multi-AZ for high availability\n- Analyze Synchronous Replication and automatic failover mechanism 2 23/09 RDS Connection \u0026amp; Operations - Configure Security Group allowing only App-Tier access\n- Customize DB config via Parameter Groups\n- Practice Automated Backups and Manual Snapshots 3 24/09 Amazon DynamoDB - Part 1 - Create DynamoDB table with Partition Key\n- Understand SQL vs NoSQL schema differences\n- Compare Provisioned vs On-Demand capacity modes 4 25/09 DynamoDB Advanced - Part 2 - Practice API operations: PutItem, GetItem, Query, Scan\n- Understand Query vs Scan efficiency\n- Create Global Secondary Index (GSI) for alternative queries 5 26/09 Amazon ElastiCache - Deploy ElastiCache with Redis engine\n- Implement Lazy Loading caching strategy\n- Place cache cluster in Private Subnet for security Week 3 Achievements: Relational Database:\nDeployed RDS with Multi-AZ for high availability Understood synchronous replication and automatic failover Configured secure access through Security Groups NoSQL Database:\nDesigned DynamoDB tables with proper key schema Understood differences between Query (efficient) vs Scan (expensive) Implemented GSI for flexible query patterns Caching Layer:\nDeployed ElastiCache Redis for application caching Applied Lazy Loading pattern to reduce RDS load Placed cache in Private Subnet for optimal latency Key Concepts:\nManaged services reduce operational burden Multi-AZ ensures business continuity Proper database selection based on use case (SQL vs NoSQL) "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.4-deploy-backend/","title":"Deploy Backend API","tags":[],"description":"","content":"Deploy Backend API with FastAPI In this section, you will set up the FastAPI backend server that orchestrates the AI agent using Strands SDK.\nStep 1: Clone or Create Project Structure Create a new directory for the backend:\nmkdir ev-rental-backend cd ev-rental-backend Project structure:\nev-rental-backend/ ‚îú‚îÄ‚îÄ app/ ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ main.py # FastAPI app ‚îÇ ‚îú‚îÄ‚îÄ agent.py # Strands Agent setup ‚îÇ ‚îú‚îÄ‚îÄ tools.py # Agent tools (search vehicles, stations) ‚îÇ ‚îî‚îÄ‚îÄ database.py # PostgreSQL connection ‚îú‚îÄ‚îÄ requirements.txt # Python dependencies ‚îú‚îÄ‚îÄ .env # Environment variables ‚îî‚îÄ‚îÄ README.md Step 2: Install Dependencies Create requirements.txt:\nfastapi==0.104.1 uvicorn[standard]==0.24.0 strands-agent-sdk==0.1.5 boto3==1.34.10 psycopg2-binary==2.9.9 sqlalchemy==2.0.23 pydantic==2.5.2 python-dotenv==1.0.0 httpx==0.25.2 Install dependencies:\n# Create virtual environment python -m venv venv # Activate virtual environment # Windows: venv\\Scripts\\activate # macOS/Linux: source venv/bin/activate # Install packages pip install -r requirements.txt Step 3: Configure Environment Variables Create .env file with your AWS credentials and Knowledge Base ID:\n# AWS Credentials AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY AWS_REGION=us-west-2 # Bedrock Configuration BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 KNOWLEDGE_BASE_ID=89CI1JSSE4 # Database Configuration DATABASE_URL=postgresql://postgres:password@localhost:5432/ev_rental_db # API Configuration BACKEND_API_URL=http://localhost:8080 ‚ö†Ô∏è Security Note:\nNever commit .env to Git Add .env to .gitignore Step 4: Create Database Models Create app/database.py:\nfrom sqlalchemy import create_engine, Column, Integer, String, Text, DateTime from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker from datetime import datetime import os DATABASE_URL = os.getenv(\u0026#34;DATABASE_URL\u0026#34;) engine = create_engine(DATABASE_URL) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() class ChatHistory(Base): __tablename__ = \u0026#34;chat_history\u0026#34; id = Column(Integer, primary_key=True, index=True) session_id = Column(String, index=True) user_message = Column(Text) agent_response = Column(Text) timestamp = Column(DateTime, default=datetime.utcnow) # Create tables Base.metadata.create_all(bind=engine) Step 5: Create Agent Tools Create app/tools.py:\nimport httpx import os from typing import List, Dict BACKEND_API_URL = os.getenv(\u0026#34;BACKEND_API_URL\u0026#34;, \u0026#34;http://localhost:8080\u0026#34;) async def search_vehicles(location: str = None, model: str = None) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for available vehicles\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: params = {} if location: params[\u0026#34;location\u0026#34;] = location if model: params[\u0026#34;model\u0026#34;] = model response = await client.get(f\u0026#34;{BACKEND_API_URL}/api/vehicles\u0026#34;, params=params) return response.json() async def search_stations(city: str = None) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for charging stations\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: params = {} if city: params[\u0026#34;city\u0026#34;] = city response = await client.get(f\u0026#34;{BACKEND_API_URL}/api/stations\u0026#34;, params=params) return response.json() Step 6: Setup Strands Agent Create app/agent.py:\nimport boto3 import os from strands_agent import Agent, Tool # Initialize Bedrock client bedrock_client = boto3.client( \u0026#39;bedrock-runtime\u0026#39;, region_name=os.getenv(\u0026#39;AWS_REGION\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;) ) # Initialize Knowledge Base client bedrock_agent_client = boto3.client( \u0026#39;bedrock-agent-runtime\u0026#39;, region_name=os.getenv(\u0026#39;AWS_REGION\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;) ) # Create Agent agent = Agent( model_id=os.getenv(\u0026#39;BEDROCK_MODEL_ID\u0026#39;), client=bedrock_client, knowledge_base_id=os.getenv(\u0026#39;KNOWLEDGE_BASE_ID\u0026#39;), tools=[ Tool( name=\u0026#34;search_vehicles\u0026#34;, description=\u0026#34;Search for available electric vehicles for rent\u0026#34;, function=search_vehicles ), Tool( name=\u0026#34;search_stations\u0026#34;, description=\u0026#34;Find nearby charging stations\u0026#34;, function=search_stations ) ] ) Step 7: Create FastAPI Application Create app/main.py:\nfrom fastapi import FastAPI, HTTPException from fastapi.middleware.cors import CORSMiddleware from pydantic import BaseModel from app.agent import agent from app.database import SessionLocal, ChatHistory import uuid app = FastAPI(title=\u0026#34;EV Rental AI Agent API\u0026#34;) # Enable CORS app.add_middleware( CORSMiddleware, allow_origins=[\u0026#34;*\u0026#34;], allow_credentials=True, allow_methods=[\u0026#34;*\u0026#34;], allow_headers=[\u0026#34;*\u0026#34;], ) class ChatRequest(BaseModel): message: str session_id: str = None class ChatResponse(BaseModel): response: str session_id: str data: dict = None @app.post(\u0026#34;/chat\u0026#34;, response_model=ChatResponse) async def chat(request: ChatRequest): try: # Generate session ID if not provided session_id = request.session_id or str(uuid.uuid4()) # Get agent response agent_response = await agent.run(request.message) # Save to database db = SessionLocal() chat_record = ChatHistory( session_id=session_id, user_message=request.message, agent_response=agent_response[\u0026#34;response\u0026#34;] ) db.add(chat_record) db.commit() db.close() return ChatResponse( response=agent_response[\u0026#34;response\u0026#34;], session_id=session_id, data=agent_response.get(\u0026#34;data\u0026#34;) ) except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @app.get(\u0026#34;/health\u0026#34;) async def health_check(): return {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;} Step 8: Run the Backend Server Start the FastAPI server:\n# Make sure virtual environment is activated uvicorn app.main:app --reload --port 8000 # You should see: # INFO: Uvicorn running on http://127.0.0.1:8000 # INFO: Application startup complete. Step 9: Test the API Test health endpoint:\ncurl http://localhost:8000/health # Response: {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Test chat endpoint:\ncurl -X POST http://localhost:8000/chat \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;Ch√≠nh s√°ch thu√™ xe c·ªßa b·∫°n l√† g√¨?\u0026#34;}\u0026#39; Expected response:\n{ \u0026#34;response\u0026#34;: \u0026#34;## üìã Ch√≠nh s√°ch thu√™ xe VinFast\\n\\n### üìÑ Gi·∫•y t·ªù c·∫ßn thi·∫øt:\\n- CMND/CCCD...\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;abc123-...\u0026#34;, \u0026#34;data\u0026#34;: null } Verification Checklist Before proceeding, ensure:\n‚úÖ Virtual environment created and activated ‚úÖ All dependencies installed ‚úÖ .env file configured with AWS credentials ‚úÖ PostgreSQL database running and connected ‚úÖ FastAPI server running on port 8000 ‚úÖ Health check endpoint returns {\u0026quot;status\u0026quot;:\u0026quot;healthy\u0026quot;} ‚úÖ Chat endpoint returns proper responses Troubleshooting Issue: \u0026ldquo;ModuleNotFoundError\u0026rdquo;\nSolution: Ensure virtual environment is activated and dependencies installed Issue: \u0026ldquo;Database connection failed\u0026rdquo;\nCheck PostgreSQL is running Verify DATABASE_URL in .env Test connection: psql -h localhost -U postgres -d ev_rental_db Issue: \u0026ldquo;Bedrock ValidationException\u0026rdquo;\nVerify AWS credentials in .env Ensure model access is granted in Bedrock console Check KNOWLEDGE_BASE_ID is correct Next: Proceed to Deploy Frontend to create the React chat interface.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Event Report \u0026ldquo;AWS Cloud Mastery Series #3: Cloud Security \u0026amp; Operations Mastery\u0026rdquo; I. EVENT OVERVIEW Event Name: AWS Cloud Mastery Series #3: Cloud Security \u0026amp; Operations Mastery\nObjectives:\nSystem Thinking: Transition from traditional infrastructure management to Cloud-Native Security model Governance Foundation: Master multi-account environment management using AWS Organizations and SCPs Defense in Depth: Implement layered security combining Identity, Network, and Data protection to eliminate Single Points of Failure Automated Response: Shift from manual incident reaction to automated remediation to minimize human latency Speaker List:\nThe event gathered top experts from the AWS community, including AWS Community Builders, Cloud Engineers, and core members of the First Cloud Journey program:\nAWS Cloud Clubs Representatives: Captains from HCMUTE, SGU, PTIT, HUFLIT (Le Vu Xuan An, Tran Duc Anh, Tran Doan Cong Ly, Danh Hoang Hieu Nghi) Identity \u0026amp; Governance Track: Huynh Hoang Long, Dinh Le Hoang Anh (AWS Community Builders) Detection \u0026amp; Monitoring Track: Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat Network Security Track: Kha Van (Cloud Security Engineer | AWS Community Builder) Data Protection Track: Thinh Lam, Viet Nguyen Incident Response Track: Mendel Grabski (Long) - ex Head of Security \u0026amp; DevOps, Tinh Truong - Platform Engineer II. KEY KNOWLEDGE INSIGHTS Based on in-depth sessions from AWS Community Builders and Cloud Security Engineers, I structured the core knowledge into the following pillars:\n1. Identity \u0026amp; Governance (Foundation) Security in the Cloud begins with controlling \u0026ldquo;Who can do what.\u0026rdquo;\nModern IAM Mindset: In the Cloud, Identity is the new Firewall.\nCredential Spectrum: Mandatory shift from Long-term Credentials (Permanent Access Keys - High Risk) to Short-term Credentials (STS tokens - Auto-expire).\nLeast Privilege: Avoid using wildcards (*) in policies.\nGovernance at Scale:\nAWS Organizations: Structure accounts into Organizational Units (OUs) like Security, Shared Services, and Workloads to isolate risks Service Control Policies (SCPs): Act as the \u0026ldquo;Constitution\u0026rdquo; of the organization. SCPs establish Guardrails (e.g., prohibiting CloudTrail disablement) that apply to all accounts, including Admins 2. Visibility \u0026amp; Detection \u0026ldquo;You cannot protect what you cannot see.\u0026rdquo;\nAmazon GuardDuty (Intelligent Scout):\nUses Machine Learning to detect anomalies based on three foundational data sources: CloudTrail, VPC Flow Logs, and DNS Logs Runtime Monitoring: Uses lightweight agent to detect deep OS-level threats like file modifications or privilege escalation AWS Security Hub (Command Center):\nSolves \u0026ldquo;alert fatigue\u0026rdquo; by normalizing findings from various tools (GuardDuty, Inspector, Macie) into a single format (ASFF) Acts as a CSPM (Cloud Security Posture Management) tool to check compliance against CIS or PCI-DSS standards 3. Network Security (The Digital Fortress) Implementing Defense-in-Depth from Edge to Core.\nVPC Fundamentals:\nSecurity Groups (Stateful): Apply Micro-segmentation using Reference IDs (e.g., SG-DB allows traffic only from SG-App) rather than whitelisting IP addresses NACLs (Stateless): A coarse filtering layer at the Subnet boundary Advanced Filtering:\nDNS Firewall (Route 53 Resolver): Blocks connections to Command \u0026amp; Control (C2) servers during domain resolution AWS Network Firewall: Provides Deep Packet Inspection (DPI) with Intrusion Prevention System (IPS) compatible with Suricata rules Modern Architecture: Utilizing AWS Transit Gateway to centralize network traffic and simplify inspection without complex \u0026ldquo;Inspection VPC\u0026rdquo; routing.\n4. Data Protection Envelope Encryption: The mechanism ensuring performance and security:\n$$Master\\ Key \\rightarrow Encrypts \\rightarrow Data\\ Key \\rightarrow Encrypts \\rightarrow Actual\\ Data$$\nSecrets Management: Replacing hardcoded credentials with AWS Secrets Manager, enabling Automatic Rotation of database passwords via Lambda.\nInfrastructure Encryption: Leveraging AWS Nitro System to offload encryption tasks to dedicated hardware, ensuring Zero Performance Impact.\n5. Incident Response Prevention Strategy: Eliminate \u0026ldquo;ClickOps\u0026rdquo; and mandate Infrastructure as Code (IaC) to prevent configuration drift.\nThe 5-Step Standard: $$Preparation \\rightarrow Detection \\rightarrow Containment \\rightarrow Eradication\\ \u0026amp;\\ Recovery \\rightarrow Post\\text{-}Incident$$\nAutomation: Using EventBridge + Lambda to automatically isolate compromised instances or remediate public buckets within seconds, racing against the speed of attacks.\nIII. EVALUATION \u0026amp; LESSONS LEARNED This workshop was a pivotal point in my understanding of Cloud Security:\n1. Shift from \u0026ldquo;Perimeter Security\u0026rdquo; to \u0026ldquo;Identity Security\u0026rdquo; I realized that the traditional concept of a \u0026ldquo;Firewall\u0026rdquo; is no longer enough. In a distributed cloud environment, Identity is the true perimeter. Managing credentials (shifting to Short-term tokens) and enforcing Least Privilege are more critical than just blocking ports.\n2. The Power of \u0026ldquo;Guardrails\u0026rdquo; over \u0026ldquo;Gatekeepers\u0026rdquo; The concept of SCPs (Service Control Policies) resonated deeply with me. Instead of acting as a \u0026ldquo;Gatekeeper\u0026rdquo; checking every developer\u0026rsquo;s action (which slows down innovation), I should act as a Platform Engineer building \u0026ldquo;Guardrails.\u0026rdquo; This allows developers to run fast within defined safety boundaries, preventing catastrophic errors (like disabling logging) programmatically.\n3. Automation is the Only Way to Win The \u0026ldquo;Sleep Better\u0026rdquo; strategy is not about hiring more security staff, but about automation. The hands-on demonstration of EventBridge + Lambda proved that humans cannot beat machine speed. Security response must be code-driven to instantly contain threats the moment they are detected.\nIV. CONCLUSION The \u0026ldquo;Cloud Security \u0026amp; Operations Mastery\u0026rdquo; series provided a comprehensive framework:\nGovernance: Starts with strict Identity management and Organizational policies Defense: Layers security across Network and Data (Encryption) Response: Relies on Automation to ensure business continuity This knowledge empowers me to not only build functional systems but to design architectures that are secure by default and resilient against modern threats.\nSome images from the event Add your images here Overall, the workshop provided a comprehensive view of modern Cloud security, from Identity \u0026amp; Governance, Visibility \u0026amp; Detection, Network Security, Data Protection to automated Incident Response, helping me build secure and sustainable systems on AWS.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in 4 events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2: From DevOps, IaC to Containers \u0026amp; Observability\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #3: Cloud Security \u0026amp; Operations Mastery\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4: Scalability, Monitoring and Content Distribution (29/09 ‚Äì 03/10) Objective: Automate system scalability, set up comprehensive monitoring, and optimize global content distribution.\nTasks completed this week: Day Date Task Activities 1 29/09 EC2 Auto Scaling - Create Launch Templates with AMI, Instance Type, Security Group\n- Set up Auto Scaling Group (Min=2, Max=4, Desired=2)\n- Configure Target Tracking Scaling Policy (CPU 50%) 2 30/09 Amazon CloudWatch Monitoring - Monitor standard metrics (CPU, Disk I/O, Network)\n- Install CloudWatch Agent for Memory metrics\n- Create Alarms with SNS notifications 3 01/10 Advanced Monitoring \u0026amp; Logs - Configure CloudWatch Agent to push application logs\n- Create CloudWatch Dashboard for system health overview\n- Explore Grafana integration 4 02/10 Amazon Route 53 DNS - Create Hosted Zone for domain management\n- Practice routing policies: Simple, Failover, Latency-based\n- Configure Health Checks for automatic failover 5 03/10 Amazon CloudFront CDN - Deploy CloudFront distribution for S3 static content\n- Configure OAC (Origin Access Control)\n- Customize TTL caching policies Week 4 Achievements: Auto Scaling:\nDeployed Auto Scaling Group with Launch Templates Tested scale out/in with stress tool Integrated with Application Load Balancer Monitoring:\nBuilt comprehensive CloudWatch Dashboard Configured alarms for CPU, Memory, Disk metrics Set up SNS notifications for alerts DNS \u0026amp; CDN:\nConfigured Route 53 with multiple routing policies Deployed CloudFront for global content distribution Implemented OAC for secure S3 access through CDN Key Concepts:\nHorizontal scaling with Auto Scaling Groups Proactive monitoring vs reactive troubleshooting Edge locations for low-latency content delivery "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.5-deploy-frontend/","title":"Deploy Frontend","tags":[],"description":"","content":"Deploying the React Frontend In this section, you will set up and run the React chat interface that connects to your FastAPI backend.\nStep 1: Clone or Create React Project Create a new React application:\n# Using Create React App npx create-react-app ev-rental-frontend cd ev-rental-frontend # Or clone existing repository git clone https://github.com/your-org/ev-rental-frontend.git cd ev-rental-frontend Step 2: Install Dependencies Install required npm packages:\n# Core dependencies npm install axios react-markdown npm install @chakra-ui/react @emotion/react @emotion/styled framer-motion npm install react-icons # Or use package.json npm install Sample package.json dependencies:\n{ \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;axios\u0026#34;: \u0026#34;^1.6.2\u0026#34;, \u0026#34;@chakra-ui/react\u0026#34;: \u0026#34;^2.8.2\u0026#34;, \u0026#34;@emotion/react\u0026#34;: \u0026#34;^11.11.1\u0026#34;, \u0026#34;@emotion/styled\u0026#34;: \u0026#34;^11.11.0\u0026#34;, \u0026#34;framer-motion\u0026#34;: \u0026#34;^10.16.16\u0026#34;, \u0026#34;react-markdown\u0026#34;: \u0026#34;^9.0.1\u0026#34;, \u0026#34;react-icons\u0026#34;: \u0026#34;^4.12.0\u0026#34; } } Step 3: Project Structure Your frontend should have this structure:\nev-rental-frontend/ ‚îú‚îÄ‚îÄ public/ ‚îÇ ‚îú‚îÄ‚îÄ index.html ‚îÇ ‚îî‚îÄ‚îÄ favicon.ico ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ App.js # Main app component ‚îÇ ‚îú‚îÄ‚îÄ index.js # Entry point ‚îÇ ‚îú‚îÄ‚îÄ components/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ChatInterface.js # Chat UI component ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ MessageList.js # Message display ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ InputBox.js # User input ‚îÇ ‚îú‚îÄ‚îÄ services/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ api.js # API calls to backend ‚îÇ ‚îú‚îÄ‚îÄ utils/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ constants.js # Configuration ‚îÇ ‚îî‚îÄ‚îÄ styles/ ‚îÇ ‚îî‚îÄ‚îÄ App.css ‚îú‚îÄ‚îÄ package.json ‚îî‚îÄ‚îÄ .env Step 4: Configure Environment Variables Create .env file in the project root:\n# .env REACT_APP_API_URL=http://localhost:8000 REACT_APP_API_BASE_PATH=/api ‚ö†Ô∏è Important: In React, environment variables must start with REACT_APP_ prefix.\nStep 5: Create API Service Create src/services/api.js:\nimport axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL || \u0026#39;http://localhost:8000\u0026#39;; const api = axios.create({ baseURL: API_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, }); export const sendMessage = async (sessionId, message) =\u0026gt; { try { const response = await api.post(\u0026#39;/api/chat\u0026#39;, { session_id: sessionId, message: message, }); return response.data; } catch (error) { console.error(\u0026#39;API Error:\u0026#39;, error); throw error; } }; export default api; Step 6: Create Chat Interface Component Create src/components/ChatInterface.js:\nimport React, { useState, useEffect, useRef } from \u0026#39;react\u0026#39;; import { Box, VStack, HStack, Input, Button, Text, Container, Heading, } from \u0026#39;@chakra-ui/react\u0026#39;; import ReactMarkdown from \u0026#39;react-markdown\u0026#39;; import { sendMessage } from \u0026#39;../services/api\u0026#39;; function ChatInterface() { const [messages, setMessages] = useState([]); const [input, setInput] = useState(\u0026#39;\u0026#39;); const [loading, setLoading] = useState(false); const [sessionId] = useState(() =\u0026gt; `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}` ); const messagesEndRef = useRef(null); const scrollToBottom = () =\u0026gt; { messagesEndRef.current?.scrollIntoView({ behavior: \u0026#39;smooth\u0026#39; }); }; useEffect(() =\u0026gt; { scrollToBottom(); }, [messages]); const handleSend = async () =\u0026gt; { if (!input.trim()) return; const userMessage = { role: \u0026#39;user\u0026#39;, content: input }; setMessages((prev) =\u0026gt; [...prev, userMessage]); setInput(\u0026#39;\u0026#39;); setLoading(true); try { const response = await sendMessage(sessionId, input); const assistantMessage = { role: \u0026#39;assistant\u0026#39;, content: response.response, data: response.data, }; setMessages((prev) =\u0026gt; [...prev, assistantMessage]); } catch (error) { const errorMessage = { role: \u0026#39;error\u0026#39;, content: \u0026#39;Failed to get response. Please try again.\u0026#39;, }; setMessages((prev) =\u0026gt; [...prev, errorMessage]); } finally { setLoading(false); } }; return ( \u0026lt;Container maxW=\u0026#34;container.md\u0026#34; py={8}\u0026gt; \u0026lt;VStack spacing={4} align=\u0026#34;stretch\u0026#34;\u0026gt; \u0026lt;Heading size=\u0026#34;lg\u0026#34;\u0026gt;üöó EV Rental AI Agent\u0026lt;/Heading\u0026gt; \u0026lt;Box border=\u0026#34;1px\u0026#34; borderColor=\u0026#34;gray.200\u0026#34; borderRadius=\u0026#34;lg\u0026#34; p={4} h=\u0026#34;500px\u0026#34; overflowY=\u0026#34;auto\u0026#34; bg=\u0026#34;gray.50\u0026#34; \u0026gt; \u0026lt;VStack spacing={3} align=\u0026#34;stretch\u0026#34;\u0026gt; {messages.map((msg, idx) =\u0026gt; ( \u0026lt;Box key={idx} alignSelf={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;flex-end\u0026#39; : \u0026#39;flex-start\u0026#39;} maxW=\u0026#34;80%\u0026#34; bg={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;blue.500\u0026#39; : \u0026#39;white\u0026#39;} color={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;white\u0026#39; : \u0026#39;black\u0026#39;} p={3} borderRadius=\u0026#34;lg\u0026#34; boxShadow=\u0026#34;sm\u0026#34; \u0026gt; {msg.role === \u0026#39;assistant\u0026#39; ? ( \u0026lt;ReactMarkdown\u0026gt;{msg.content}\u0026lt;/ReactMarkdown\u0026gt; ) : ( \u0026lt;Text\u0026gt;{msg.content}\u0026lt;/Text\u0026gt; )} \u0026lt;/Box\u0026gt; ))} {loading \u0026amp;\u0026amp; ( \u0026lt;Box alignSelf=\u0026#34;flex-start\u0026#34; maxW=\u0026#34;80%\u0026#34;\u0026gt; \u0026lt;Text color=\u0026#34;gray.500\u0026#34;\u0026gt;Typing...\u0026lt;/Text\u0026gt; \u0026lt;/Box\u0026gt; )} \u0026lt;div ref={messagesEndRef} /\u0026gt; \u0026lt;/VStack\u0026gt; \u0026lt;/Box\u0026gt; \u0026lt;HStack\u0026gt; \u0026lt;Input value={input} onChange={(e) =\u0026gt; setInput(e.target.value)} onKeyPress={(e) =\u0026gt; e.key === \u0026#39;Enter\u0026#39; \u0026amp;\u0026amp; handleSend()} placeholder=\u0026#34;Ask about vehicle rentals, policies, or charging stations...\u0026#34; disabled={loading} /\u0026gt; \u0026lt;Button onClick={handleSend} colorScheme=\u0026#34;blue\u0026#34; isLoading={loading} disabled={loading} \u0026gt; Send \u0026lt;/Button\u0026gt; \u0026lt;/HStack\u0026gt; \u0026lt;/VStack\u0026gt; \u0026lt;/Container\u0026gt; ); } export default ChatInterface; Step 7: Update App.js Update src/App.js:\nimport React from \u0026#39;react\u0026#39;; import { ChakraProvider } from \u0026#39;@chakra-ui/react\u0026#39;; import ChatInterface from \u0026#39;./components/ChatInterface\u0026#39;; function App() { return ( \u0026lt;ChakraProvider\u0026gt; \u0026lt;ChatInterface /\u0026gt; \u0026lt;/ChakraProvider\u0026gt; ); } export default App; Step 8: Run the Frontend Start the React development server:\nnpm start Expected output:\nCompiled successfully! You can now view ev-rental-frontend in the browser. Local: http://localhost:3000 On Your Network: http://192.168.1.10:3000 The application will automatically open in your browser at http://localhost:3000.\nStep 9: Test the Chat Interface Try these sample queries:\nKnowledge Base Query:\n\u0026ldquo;Ch√≠nh s√°ch thu√™ xe l√† g√¨?\u0026rdquo; \u0026ldquo;T√¥i c·∫ßn gi·∫•y t·ªù g√¨ ƒë·ªÉ thu√™ xe?\u0026rdquo; Vehicle Search:\n\u0026ldquo;T√¨m xe VinFast VF8 ·ªü H√† N·ªôi t·ª´ ng√†y 20/12\u0026rdquo; \u0026ldquo;C√≥ xe n√†o available?\u0026rdquo; Charging Station:\n\u0026ldquo;Tr·∫°m s·∫°c g·∫ßn Ho√†n Ki·∫øm\u0026rdquo; \u0026ldquo;T√¨m tr·∫°m s·∫°c ·ªü Qu·∫≠n 1\u0026rdquo; Verification Checklist Before proceeding, ensure:\n‚úÖ Node.js and npm installed ‚úÖ All dependencies installed successfully ‚úÖ .env file configured with backend URL ‚úÖ Backend server running on port 8000 ‚úÖ Frontend running on port 3000 ‚úÖ Chat interface loads without errors ‚úÖ Can send messages and receive responses ‚úÖ Markdown formatting displays correctly Troubleshooting Issue: \u0026ldquo;Module not found\u0026rdquo;\nSolution: Delete node_modules and run npm install again Check package.json for correct versions Issue: \u0026ldquo;Network Error\u0026rdquo; when sending messages\nCheck backend is running: curl http://localhost:8000/health Verify REACT_APP_API_URL in .env Check browser console for CORS errors Issue: \u0026ldquo;CORS policy error\u0026rdquo;\nEnsure backend has CORS middleware configured Check allow_origins includes http://localhost:3000 Issue: Port 3000 already in use\nChange port: PORT=3001 npm start Or kill existing process Issue: Markdown not rendering\nVerify react-markdown is installed Check import statement in ChatInterface.js Next: Proceed to Testing to verify all features work correctly.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5: Operations and Infrastructure as Code (06/10 - 10/10) Objective: Eliminate manual operations, manage server fleets at scale, and codify infrastructure.\nTasks completed this week: Day Date Task Activities 1 06/10 AWS Systems Manager - Configure SSM Agent with IAM Role\n- Run Command on multiple instances\n- Collect software inventory for auditing 2 07/10 Session Manager Secure Access - Close port 22 (SSH) on Security Groups\n- Access via HTTPS Session Manager\n- Log all sessions to S3/CloudWatch 3 08/10 CloudFormation IaC - Write YAML templates for VPC and EC2\n- Practice Stack creation, update, delete\n- Use Change Sets and Drift Detection 4 09/10 AWS CDK Basics - Use TypeScript/Python for infrastructure\n- Learn Construct levels (L1, L2, L3)\n- Create VPC with single line of code 5 10/10 AWS CDK Advanced \u0026amp; Workshop - Build reusable custom Constructs\n- Handle circular dependencies\n- Manage environment variables with Context Week 5 Achievements: Systems Management:\nEliminated SSH key management with Session Manager Ran commands on fleet without individual access Collected inventory for compliance auditing Infrastructure as Code:\nWrote CloudFormation templates in YAML Understood Stack lifecycle and state management Detected configuration drift AWS CDK:\nUsed programming language for infrastructure Built reusable Constructs for organization Completed IaC Workshop challenges Key Concepts:\nIaC eliminates manual configuration errors Session Manager is the gold standard for server access CDK provides higher abstraction than CloudFormation "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building an EV Rental AI Agent with AWS Bedrock Overview EV Rental AI Agent is an intelligent chatbot built to assist customers in the VinFast electric vehicle rental system. This workshop demonstrates how to leverage AWS Bedrock, Claude 3.5 Sonnet, and Knowledge Bases to create a conversational AI that can:\nAnswer natural language questions in Vietnamese Automatically search information from multiple sources Display data as interactive cards in the chat interface Retrieve available vehicles and charging stations Access rental policies and FAQs from a knowledge base In this workshop, you will learn how to:\nSetup AWS Bedrock - Enable AI models and create a Knowledge Base for document retrieval Deploy Backend API - Build a FastAPI server with Strands Agent SDK for intelligent tool selection Deploy Frontend - Create a React chat interface with Chakra UI components Test the System - Interact with the AI agent and verify all functionalities Content Workshop Overview Prerequisites Setup AWS Bedrock Deploy Backend API Deploy Frontend Testing the AI Agent Clean Up Resources "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Self-Assessment During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 08/09 to 12/12, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the VoltGo (EV Station-based Rental System) project as a Frontend Developer (developing Web Dashboard interface). Through this, I significantly improved the following skills:\nProgramming: Building management interface with Next.js, deploying through AWS Amplify, and handling API integration. Analysis \u0026amp; Integration: Deep understanding of data flow between Frontend and AWS services. Report Writing \u0026amp; Documentation: Skills in documenting components and deployment processes. Regarding work attitude, I always strive to complete assigned professional tasks. However, I recognize that I need to seriously improve my discipline and work processes to achieve optimal efficiency.\nTo objectively reflect on my internship process, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge (Next.js, AWS) in practice, code quality ‚úÖ 2 Ability to learn Absorbing new technology, learning quickly ‚úÖ 3 Proactiveness Self-learning, taking on tasks without waiting for instructions ‚úÖ 4 Sense of responsibility Completing work on time, ensuring quality ‚úÖ 5 Discipline Adhering to schedules, regulations, work processes ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ 7 Communication Presenting ideas, reporting work clearly ‚úÖ 8 Teamwork Working effectively with colleagues, participating in teams ‚úÖ 9 Professional conduct Respecting colleagues, partners, work environment ‚úÖ 10 Problem-solving skills Identifying problems, proposing solutions, creativity ‚úÖ 11 Contribution to project/organization Work effectiveness, innovative ideas, recognition from team ‚úÖ 12 Overall General evaluation of the entire internship process ‚úÖ Needs Improvement Based on my actual work experience, I recognize that I need to focus on overcoming the following 3 weaknesses to improve myself:\nEnhancing discipline and compliance with processes:\nCurrent situation: Sometimes negligent in strictly adhering to administrative regulations or company schedules. Action: I commit to cultivating professional work habits, strictly complying with AWS regulations as well as any future organization, viewing this as the core foundation of professionalism. Improving problem-solving mindset:\nCurrent situation: When the system encounters complex bugs, I often handle them with temporary fixes rather than in-depth analysis. Action: I will cultivate systematic logical thinking, learn to analyze root causes (Root Cause Analysis) before fixing code to provide optimal and sustainable solutions. Learning to communicate more effectively:\nCurrent situation: Communication skills in daily work and handling situations are sometimes awkward and not concise. Action: I will actively participate in group discussions, learn to listen and present issues concisely and clearly so that information is conveyed most accurately to colleagues. "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.6-testing/","title":"Testing the System","tags":[],"description":"","content":"Testing the EV Rental AI Agent In this section, you will test all three core features of the AI Agent to ensure everything works correctly.\nPrerequisites for Testing Before testing, ensure:\n‚úÖ Backend server running on http://localhost:8000 ‚úÖ Frontend application running on http://localhost:3000 ‚úÖ PostgreSQL database is running and populated with test data ‚úÖ AWS Bedrock Knowledge Base is synced and ready Test Scenario 1: Knowledge Base Search The AI Agent should be able to answer questions about rental policies, pricing, and FAQs using the Knowledge Base.\nTest Queries:\nRental Policy:\nUser: \u0026#34;Ch√≠nh s√°ch thu√™ xe l√† g√¨?\u0026#34; Expected: Agent returns rental policy details from Knowledge Base Required Documents:\nUser: \u0026#34;T√¥i c·∫ßn gi·∫•y t·ªù g√¨ ƒë·ªÉ thu√™ xe?\u0026#34; Expected: Agent lists required documents (ID, license, deposit info) Pricing Information:\nUser: \u0026#34;Gi√° thu√™ xe VinFast VF8 l√† bao nhi√™u?\u0026#34; Expected: Agent provides pricing details from Knowledge Base Booking Process:\nUser: \u0026#34;L√†m th·∫ø n√†o ƒë·ªÉ ƒë·∫∑t xe?\u0026#34; Expected: Agent explains step-by-step booking process Verification:\n‚úÖ Response includes citation from Knowledge Base ‚úÖ Answer is relevant and accurate ‚úÖ Markdown formatting displays correctly ‚úÖ Response time is under 5 seconds Test Scenario 2: Vehicle Search The AI Agent should search the PostgreSQL database for available vehicles based on location and date.\nTest Queries:\nSearch by Location:\nUser: \u0026#34;T√¨m xe ·ªü H√† N·ªôi\u0026#34; Expected: Agent lists available vehicles in Hanoi Search by Model:\nUser: \u0026#34;C√≥ xe VinFast VF8 n√†o available kh√¥ng?\u0026#34; Expected: Agent shows VF8 vehicles with availability status Search with Date Range:\nUser: \u0026#34;T√¨m xe VF9 ·ªü H·ªì Ch√≠ Minh t·ª´ ng√†y 20/12 ƒë·∫øn 25/12\u0026#34; Expected: Agent searches vehicles available in that date range Search with Price Range:\nUser: \u0026#34;Xe n√†o d∆∞·ªõi 1 tri·ªáu ƒë·ªìng/ng√†y?\u0026#34; Expected: Agent filters vehicles by price Verification:\n‚úÖ Agent correctly extracts search parameters (location, model, dates) ‚úÖ Results include vehicle details (model, price, location, availability) ‚úÖ Data is fetched from PostgreSQL database ‚úÖ Results are formatted in a readable table or list Expected Response Format:\n## üöó Available Vehicles | Model | Location | Price/Day | Status | |-------|----------|-----------|--------| | VinFast VF8 | H√† N·ªôi | 800,000ƒë | Available | | VinFast VF9 | H√† N·ªôi | 1,200,000ƒë | Available | Test Scenario 3: Charging Station Finder The AI Agent should find nearby charging stations with real-time availability.\nTest Queries:\nSearch by District:\nUser: \u0026#34;Tr·∫°m s·∫°c g·∫ßn Qu·∫≠n Ho√†n Ki·∫øm\u0026#34; Expected: Agent lists charging stations in Hoan Kiem district Search by Address:\nUser: \u0026#34;T√¨m tr·∫°m s·∫°c ·ªü Qu·∫≠n 1, TP.HCM\u0026#34; Expected: Agent finds stations in District 1, HCMC Check Station Availability:\nUser: \u0026#34;Tr·∫°m s·∫°c n√†o c√≤n tr·ªëng?\u0026#34; Expected: Agent shows stations with available charging ports Filter by Connector Type:\nUser: \u0026#34;Tr·∫°m s·∫°c c√≥ CCS2 connector\u0026#34; Expected: Agent filters stations with CCS2 connectors Verification:\n‚úÖ Agent correctly identifies location from query ‚úÖ Results include station name, address, and availability ‚úÖ Connector types are listed ‚úÖ Real-time availability status is shown Expected Response Format:\n## ‚ö° Charging Stations Near You ### VinFast Station - Ho√†n Ki·∫øm üìç Address: 123 Tr·∫ßn H∆∞ng ƒê·∫°o, Ho√†n Ki·∫øm, H√† N·ªôi üîå Connectors: CCS2 (2 available), CHAdeMO (1 available) ‚è∞ Hours: 24/7 ‚úÖ Status: Available Test Scenario 4: Multi-Turn Conversations Test the agent\u0026rsquo;s ability to maintain context across multiple turns.\nTest Conversation:\nUser: \u0026#34;T√¥i mu·ªën thu√™ xe VF8\u0026#34; Agent: [Provides VF8 information] User: \u0026#34;Gi√° bao nhi√™u?\u0026#34; Agent: [Should understand context refers to VF8 pricing] User: \u0026#34;Tr·∫°m s·∫°c g·∫ßn ƒë√≥ ·ªü ƒë√¢u?\u0026#34; Agent: [Should find charging stations near VF8 location] Verification:\n‚úÖ Agent maintains conversation context ‚úÖ Pronouns and references are resolved correctly ‚úÖ Session ID persists across messages Test Scenario 5: Error Handling Test how the agent handles invalid or unclear queries.\nTest Cases:\nAmbiguous Query:\nUser: \u0026#34;Xe\u0026#34; Expected: Agent asks for clarification Unavailable Vehicle:\nUser: \u0026#34;T√¨m xe Tesla\u0026#34; Expected: Agent explains Tesla is not available, suggests alternatives Invalid Date:\nUser: \u0026#34;Thu√™ xe t·ª´ ng√†y 32/13\u0026#34; Expected: Agent detects invalid date and asks for correction Out of Scope:\nUser: \u0026#34;What\u0026#39;s the weather today?\u0026#34; Expected: Agent politely explains it can only help with EV rentals Verification:\n‚úÖ Agent handles errors gracefully ‚úÖ Provides helpful error messages ‚úÖ Suggests alternatives when possible Performance Testing Check system performance under normal usage:\nMetrics to Monitor:\nResponse Time:\nKnowledge Base queries: \u0026lt; 3 seconds Vehicle search: \u0026lt; 2 seconds Charging station search: \u0026lt; 2 seconds API Health:\ncurl http://localhost:8000/health Expected: 200 OK with health status\nBackend Logs: Check for errors in FastAPI console output\nFrontend Console: Open browser DevTools ‚Üí Console\nNo JavaScript errors API calls succeed (Network tab) Integration Testing Checklist Run through this comprehensive checklist:\n‚úÖ Knowledge Base Integration:\nAgent can retrieve policy information Citations are included in responses Bedrock API calls succeed ‚úÖ Database Integration:\nVehicle search queries PostgreSQL Results are accurate and up-to-date Database connection is stable ‚úÖ Backend API:\n/api/chat endpoint works /health endpoint responds Session management functions correctly ‚úÖ Frontend UI:\nMessages display correctly User input is captured Loading states work Markdown renders properly Auto-scroll functions ‚úÖ Error Handling:\nNetwork errors are caught Invalid inputs handled gracefully User receives helpful feedback Testing with Postman (Optional) Test backend API directly:\n1. Health Check:\nGET http://localhost:8000/health 2. Chat Request:\nPOST http://localhost:8000/api/chat Content-Type: application/json { \u0026#34;session_id\u0026#34;: \u0026#34;test-session-123\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Ch√≠nh s√°ch thu√™ xe l√† g√¨?\u0026#34; } Expected Response:\n{ \u0026#34;response\u0026#34;: \u0026#34;## üìã Ch√≠nh s√°ch thu√™ xe VinFast\\n\\n...\u0026#34;, \u0026#34;data\u0026#34;: null, \u0026#34;session_id\u0026#34;: \u0026#34;test-session-123\u0026#34; } Troubleshooting Test Failures Issue: Knowledge Base returns empty results\nCheck Knowledge Base is synced in AWS Console Verify KNOWLEDGE_BASE_ID in .env Test KB directly in Bedrock console Issue: Vehicle search returns no results\nCheck PostgreSQL database has test data Verify DATABASE_URL connection string Run SQL query directly: SELECT * FROM vehicles; Issue: Charging stations not found\nVerify backend API /stations endpoint works Check station data in database Test API call: curl http://localhost:8080/stations Issue: Frontend not connecting to backend\nCheck REACT_APP_API_URL in frontend .env Verify backend CORS allows http://localhost:3000 Check browser console for network errors Test Report Template Document your test results:\n## Test Report - EV Rental AI Agent **Date:** 2024-12-20 **Tester:** Your Name ### Test Results Summary - Total Tests: 15 - Passed: 14 - Failed: 1 - Success Rate: 93% ### Detailed Results #### Knowledge Base Search - [x] Rental policy query - PASS - [x] Required documents - PASS - [x] Pricing information - PASS - [ ] Booking process - FAIL (slow response) #### Vehicle Search - [x] Search by location - PASS - [x] Search by model - PASS - [x] Date range search - PASS #### Charging Station Finder - [x] District search - PASS - [x] Availability check - PASS ### Issues Found 1. Booking process query takes 7 seconds (\u0026gt; 5s threshold) - Root cause: Knowledge Base sync incomplete - Fix: Re-sync data source ### Recommendations - Monitor response times during peak usage - Add caching for frequently asked questions - Implement rate limiting Success! üéâ Your EV Rental AI Agent is now fully tested and operational.\nNext: Proceed to Cleanup to remove resources and avoid charges.\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6: Multi-layer Security Architecture (13/10 - 17/10) Objective: Build a digital fortress to protect data and applications from modern cyber threats.\nTasks completed this week: Day Date Task Activities 1 13/10 AWS WAF - Deploy WAF on ALB/CloudFront\n- Configure rules against SQL Injection, XSS\n- Create Rate-based rules for DDoS protection 2 14/10 AWS KMS Encryption - Create Customer Managed Keys (CMK)\n- Configure Key Policy for admin/user separation\n- Enable encryption on EBS and S3 3 15/10 Secrets Manager - Store RDS passwords in Secrets Manager\n- Configure automatic rotation with Lambda\n- Update application to retrieve secrets via API 4 16/10 GuardDuty Threat Detection - Enable intelligent threat detection\n- Analyze CloudTrail, VPC Flow Logs, DNS Logs\n- Configure EventBridge alerts for high-severity findings 5 17/10 Amazon Cognito - Create User Pool for app authentication\n- Configure Identity Pool for temporary AWS credentials\n- Enable direct S3 upload from end users Week 6 Achievements: Application Security:\nDeployed WAF protecting against OWASP Top 10 Implemented rate limiting for DDoS mitigation Data Encryption:\nCreated and managed CMK with proper key policies Encrypted data at rest across EBS, S3, RDS Secrets Management:\nEliminated hardcoded credentials in applications Automated password rotation Threat Detection:\nEnabled ML-based threat detection with GuardDuty Set up automated alerting for security incidents Identity Federation:\nBuilt user authentication with Cognito User Pools Enabled federated access with Identity Pools "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/5-workshop/5.7-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleaning Up Resources After completing the workshop, follow these steps to clean up all resources and avoid unnecessary AWS charges.\nWhy Cleanup is Important Cost Savings: AWS charges for active resources like Bedrock Knowledge Bases, S3 storage, and running services Security: Remove unused IAM credentials to maintain security best practices Organization: Keep your AWS account clean and organized Step 1: Delete AWS Bedrock Knowledge Base 1.1 Delete Knowledge Base Open the AWS Bedrock Console Navigate to Knowledge Bases in the left sidebar Select your Knowledge Base: ev-rental-knowledge-base Click Delete Confirm deletion by typing the Knowledge Base name Click Delete to confirm ‚ö†Ô∏è Note: This will also delete the associated data source connections.\n1.2 Delete S3 Bucket and Documents Open the S3 Console Find your bucket: ev-rental-knowledge-docs Select the bucket Click Empty to delete all objects Confirm by typing \u0026ldquo;permanently delete\u0026rdquo; After emptying, click Delete on the bucket Confirm by typing the bucket name Or use AWS CLI:\n# Delete all objects in bucket aws s3 rm s3://ev-rental-knowledge-docs --recursive # Delete the bucket aws s3 rb s3://ev-rental-knowledge-docs Step 2: Delete IAM User and Access Keys 2.1 Delete Access Keys Open the IAM Console Navigate to Users Select your user (e.g., bedrock-agent-user) Click on the Security credentials tab Under Access keys, find your access key Click Delete next to the access key Confirm deletion 2.2 Delete IAM User (Optional) If you created a dedicated IAM user for this workshop:\nIn the IAM Console, select the user Click Delete user Confirm by checking the box Click Delete Or use AWS CLI:\n# List access keys aws iam list-access-keys --user-name bedrock-agent-user # Delete access key (replace with your key ID) aws iam delete-access-key --user-name bedrock-agent-user --access-key-id AKIA5GPEMGJZK6E7PMEB # Delete user aws iam delete-user --user-name bedrock-agent-user Step 3: Stop Local Services 3.1 Stop FastAPI Backend In the terminal where FastAPI is running:\nPress Ctrl + C to stop the server\nDeactivate virtual environment:\ndeactivate Optionally delete the project folder:\n# On macOS/Linux rm -rf ev-rental-backend # On Windows rmdir /s ev-rental-backend 3.2 Stop React Frontend In the terminal where React is running:\nPress Ctrl + C to stop the development server\nOptionally delete the project folder:\n# On macOS/Linux rm -rf ev-rental-frontend # On Windows rmdir /s ev-rental-frontend 3.3 Stop PostgreSQL Database If you installed PostgreSQL locally for this workshop:\nOn macOS:\n# Stop PostgreSQL service brew services stop postgresql@14 On Linux:\nsudo systemctl stop postgresql On Windows:\n# Open Services (services.msc) # Find \u0026#34;PostgreSQL\u0026#34; service # Right-click ‚Üí Stop 3.4 Delete Database (Optional) If you want to completely remove the database:\n# Connect to PostgreSQL psql -U postgres # Drop database DROP DATABASE ev_rental_db; # Exit \\q Step 4: Remove Environment Files Delete sensitive .env files that contain credentials:\nBackend:\ncd ev-rental-backend rm .env Frontend:\ncd ev-rental-frontend rm .env ‚ö†Ô∏è Security Note: Never commit .env files to Git. Always add them to .gitignore.\nStep 5: Verify Cleanup 5.1 Check AWS Resources Verify all resources are deleted:\nBedrock Console:\nNo Knowledge Bases listed No model invocations active S3 Console:\nBucket ev-rental-knowledge-docs is deleted IAM Console:\nAccess keys are deleted IAM user removed (if you chose to delete it) 5.2 Check AWS Costs Open the AWS Billing Console Check Bills for the current month Verify charges: Bedrock charges should stop after Knowledge Base deletion S3 storage charges should stop after bucket deletion No ongoing compute charges Or use AWS CLI:\naws ce get-cost-and-usage \\ --time-period Start=2024-12-01,End=2024-12-31 \\ --granularity MONTHLY \\ --metrics UnblendedCost \\ --group-by Type=SERVICE Cost Breakdown Here\u0026rsquo;s what you may have been charged during the workshop:\nService Estimated Cost Notes AWS Bedrock - Claude 3.5 Sonnet ~$0.50 - $2.00 Depends on number of queries AWS Bedrock - Knowledge Base ~$0.10 - $0.50 Vector storage and retrieval S3 Storage ~$0.02 Minimal for small documents Data Transfer ~$0.05 Usually within free tier Total ~$0.67 - $2.57 Approximate for workshop ‚ö†Ô∏è Note: Most costs come from Bedrock API calls. The longer you test, the higher the cost.\nCleanup Checklist Before you finish, verify all items are complete:\nAWS Resources ‚úÖ Bedrock Knowledge Base deleted ‚úÖ S3 bucket emptied and deleted ‚úÖ IAM Access Keys deleted ‚úÖ IAM User deleted (optional) Local Resources ‚úÖ FastAPI backend stopped ‚úÖ React frontend stopped ‚úÖ PostgreSQL database stopped ‚úÖ PostgreSQL database dropped (optional) Sensitive Files ‚úÖ Backend .env file deleted ‚úÖ Frontend .env file deleted ‚úÖ No AWS credentials in project files Verification ‚úÖ AWS Console shows no active resources ‚úÖ Billing dashboard shows stopped charges ‚úÖ Local services not running Troubleshooting Cleanup Issues Issue: Cannot delete S3 bucket - \u0026ldquo;Bucket not empty\u0026rdquo;\nSolution: Empty all objects first using the S3 Console or CLI Command: aws s3 rm s3://bucket-name --recursive Issue: Cannot delete Knowledge Base - \u0026ldquo;In use\u0026rdquo;\nSolution: Wait a few minutes for any pending operations to complete Check if any API calls are still referencing it Issue: IAM User deletion fails - \u0026ldquo;User has attached policies\u0026rdquo;\nSolution: Detach all policies first Go to IAM ‚Üí Users ‚Üí Select user ‚Üí Permissions ‚Üí Detach policies Issue: PostgreSQL won\u0026rsquo;t stop\nSolution: Force kill the process On macOS/Linux: sudo killall postgres On Windows: Use Task Manager to end PostgreSQL processes Optional: Keep Learning If you want to continue experimenting:\nKeep These Resources: ‚úÖ IAM User (with minimal permissions) ‚úÖ Bedrock model access (no charge when not in use) What You Can Do Next: Add more documents to Knowledge Base Implement additional agent tools Deploy to AWS Lambda for serverless operation Add authentication and user management Integrate with real vehicle booking systems Conclusion üéâ Congratulations! You have successfully:\n‚úÖ Built an AI Agent using AWS Bedrock and Claude 3.5 Sonnet ‚úÖ Integrated Knowledge Bases for intelligent document retrieval ‚úÖ Created a FastAPI backend with Strands Agent SDK ‚úÖ Developed a React frontend for user interaction ‚úÖ Tested all features end-to-end ‚úÖ Cleaned up resources to avoid charges Key Takeaways: AI Agents can autonomously select tools and make decisions AWS Bedrock simplifies access to foundation models like Claude Knowledge Bases enable semantic search over documents Strands SDK provides a framework for building agent workflows FastAPI + React create a modern full-stack AI application Next Steps: Explore other Bedrock models (Llama 3, Mistral, etc.) Learn about RAG (Retrieval Augmented Generation) Build more complex agent workflows Deploy to production using AWS services "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program, helping the FCJ team improve any shortcomings based on the following aspects:\nI. Overall Evaluation 1. Working Environment\nThe environment at FCJ is very professional and technology-oriented (\u0026ldquo;Builder culture\u0026rdquo;). I was impressed by the openness in communication, where interns are encouraged to express opinions on technical solutions rather than just following instructions. The workspace stimulates creativity, and access to AWS Cloud resources is always available. However, due to the high-tech nature of the project, the work atmosphere can sometimes be tense. I think having some short \u0026ldquo;Happy Hour\u0026rdquo; activities mid-week would help everyone relieve pressure better.\n2. Support from Mentor / Team Admin\nMentors play a very important role in my development. As a Frontend role requiring integration with many complex AWS services, I frequently encountered difficulties. The mentor was very patient, not only pointing out errors but also guiding me on how to think to find root causes (Root Cause Analysis). The Admin team supports processes very efficiently, helping me focus entirely on my professional work.\n3. Relevance of Work to Academic Major\nThe Frontend Developer role in the VoltGo project is completely aligned with my major but at a much higher \u0026ldquo;hands-on\u0026rdquo; level. From only knowing how to write simple interface code, I learned to think about System Integration, security with Cognito, and optimizing user experience on the Cloud platform. This is a knowledge foundation that schools cannot fully provide.\n4. Learning \u0026amp; Skill Development Opportunities\nThis is the period when I learned the most so far. Professionally, I became more proficient with React Native, Next.js, and the AWS ecosystem. In terms of soft skills, I learned Agile work processes, source code management, and most importantly, understood how the standards of a commercial product (stability, scalability) differ greatly from student projects.\n5. Company Culture \u0026amp; Team Spirit\nThe spirits of \u0026ldquo;Customer Obsession\u0026rdquo; and \u0026ldquo;Ownership\u0026rdquo; from AWS are clearly spread throughout the team. People support each other regardless of rank. There were times when I made mistakes affecting the overall progress, but instead of blaming me, the whole team rallied to help me fix bugs. This made me feel accepted and motivated to improve my discipline.\n6. Internship Policies / Benefits\nFor me, the most valuable benefit here is not material, but the opportunity to \u0026lsquo;immerse myself\u0026rsquo; in an environment of top Cloud experts. Working daily alongside experienced Mentors, directly observing how they think and solve tricky problems are invaluable \u0026lsquo;professional\u0026rsquo; lessons that books cannot teach. This is truly a \u0026lsquo;gold mine\u0026rsquo; of knowledge that significantly shortens my self-learning time.\nII. Additional Questions What did you find most satisfying during your internship?\nThe feeling of \u0026ldquo;Ownership\u0026rdquo;. I was given real responsibility for important features on the App and Web Dashboard, the code I wrote runs on actual systems, not just simulated exercises.\nWhat do you think the company should improve for future interns?\nI think the program should add more soft-skill training sessions (\u0026ldquo;Soft-skill workshops\u0026rdquo;) from the beginning of the term, especially on Time Management and Workplace Discipline. Many technical students like me, when first entering, are often overwhelmed and don\u0026rsquo;t know how to organize work scientifically, leading to unnecessary pressure.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nDefinitely YES. This is an ideal environment to \u0026ldquo;push\u0026rdquo; yourself to mature. Although the pressure is high, the knowledge about Cloud and professional work processes gained after 3 months will be invaluable baggage for your career later.\nIII. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest Mentors could establish periodic \u0026ldquo;Check-point 1:1\u0026rdquo; sessions every 2 weeks not only to review code but also to review attitude and work style. Receiving early and straightforward feedback on discipline or communication will help interns recognize weaknesses and adjust promptly before the end of the internship.\nWould you like to continue this program in the future?\nI very much hope to have the opportunity to return to AWS or continue working with FCJ in a higher position after I have improved my lacking skills and graduated.\nAny other comments (free sharing):\nThank you to the FCJ Team and Mentors for creating such a beneficial playground. Although I still have many limitations in terms of discipline and communication during this time, everyone\u0026rsquo;s patience is the greatest lesson I have received. Wishing the program continued success!\nClosing my journey at FCJ, I want to express my deepest gratitude to the dedicated mentors who have helped me complete this journey fully.\nSpecial thanks to brothers: Nguy·ªÖn Gia H∆∞ng, VƒÉn Ho√†ng Kha, and all the mentors. Thank you for always supporting me enthusiastically, day and night, from the smallest processes to guiding strategic thinking.\nYour patience and encouragement are the most valuable gifts I have received. Wishing the Admin and Mentor team of FCJ good health and success!\n"},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7: Migration \u0026amp; Disaster Recovery (20/10 - 24/10) Date Day Topic Detailed Activities 20/10 Mon AWS Migration Hub - Review migration strategies (6R\u0026rsquo;s)\n- Compare Lift-and-Shift vs Replatforming\n- Assess migration readiness 21/10 Tue Database Migration - DMS setup for database migration\n- Schema Conversion Tool\n- CDC for ongoing replication 22/10 Wed Application Migration - AWS Application Migration Service\n- Server replication setup\n- Test cutover procedures 23/10 Thu Disaster Recovery - Design multi-region DR architecture\n- Pilot Light vs Warm Standby\n- Route 53 health checks and failover 24/10 Fri Backup \u0026amp; Recovery - AWS Backup for centralized management\n- Cross-region backup replication\n- Recovery testing and validation Week 7 Achievements: Understood all 6 migration strategies and when to use each Hands-on experience with DMS and Schema Conversion Tool Designed DR architecture with RTO/RPO requirements Implemented centralized backup strategy with AWS Backup "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8: AWS Cost Optimization \u0026amp; Voltgo Project Kickoff (27/10 - 31/10) Dual Objective: Continue learning cost optimization and advanced networking on AWS, while starting the Voltgo Frontend project.\nPART A: AWS - Cost Optimization and Advanced Networking Day Date Task Activities 1 27/10 Cost Explorer \u0026amp; CUR - Analyze costs by Service, Region, Tag\n- Enable Cost and Usage Report (CUR)\n- Query CUR with Glue and Athena 2 28/10 Right-Sizing - Use Compute Optimizer for recommendations\n- Integrate CloudWatch Agent for memory metrics 3 29/10 Savings Plans \u0026amp; RIs - Compare EC2 Instance vs Compute Savings Plans\n- Plan 1-year commitment for base load 4 30/10 Transit Gateway - Deploy hub-and-spoke network architecture\n- Configure routing tables between VPCs 5 31/10 VPC Flow Logs - Enable network traffic analysis\n- Debug REJECT status to identify blocking rules PART B: Voltgo Project - Frontend Developer Role From Week 8 onwards, I joined the Voltgo project - an electric vehicle rental platform. As a Frontend Developer, I am responsible for building main interfaces: User, Login, Booking, Blog, Vehicle, Station.\nDay Date Task Activities 1 27/10 Project Kickoff - Understand Voltgo business requirements\n- Identify 6 main modules: User, Login, Booking, Blog, Vehicle, Station\n- Tech stack: React.js + TypeScript 2 28/10 Dev Environment Setup - Initialize project with Vite + TypeScript\n- Feature-based folder structure\n- Git branching strategy (feature/user, feature/booking\u0026hellip;) 3 29/10 Design System - Define Voltgo theme colors (green for eco-friendly)\n- Build shared components: VehicleCard, StationMarker, BookingCard 4 30/10 Routing \u0026amp; State Management - Configure React Router v6 for all pages\n- Set up Redux Toolkit slices for each module\n- Create Axios API layer 5 31/10 Layout Components - Build MainLayout with Header, Sidebar, Footer\n- Implement responsive navigation menu\n- Mobile hamburger menu Week 8 Achievements: AWS Cost Optimization:\nAnalyzed costs with Cost Explorer and CUR Identified over-provisioned resources with Compute Optimizer Planned Savings Plans for cost reduction Voltgo Frontend:\nProject initialized with React + TypeScript Design System defined with Voltgo branding Core layout and routing structure completed Ready for feature development from Week 9 "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9: AWS Containers \u0026amp; Voltgo Login/User Modules (03/11 - 07/11) Dual Objective: Master Docker and container orchestration platforms on AWS, while building Authentication pages for Voltgo.\nPART A: AWS - Container Technology Day Date Task Activities 1 03/11 Docker Fundamentals - Write optimized Dockerfile (Multi-stage build)\n- Create ECR Repository\n- Push private image to ECR 2 04/11 Amazon ECS \u0026amp; Fargate - Create Task Definition with CPU/RAM specs\n- Deploy ECS Service with Fargate\n- Integrate with Application Load Balancer 3 05/11 Amazon EKS Setup - Create EKS cluster with eksctl\n- Configure Managed Node Groups\n- AWS handles patching and upgrades 4 06/11 Deploy on EKS - Write K8s YAML manifests (Deployment, Service, Ingress)\n- Install AWS Load Balancer Controller\n- Auto-create ALB from Ingress 5 07/11 CI/CD for Containers - Build CI/CD Pipeline for ECS/EKS\n- CodeCommit -\u0026gt; CodeBuild -\u0026gt; CodeDeploy\n- Blue/Green Deployment strategy PART B: Voltgo - Login \u0026amp; User Modules Day Date Task Activities 1 03/11 Login Page - Build Login UI with React Hook Form + Yup validation\n- Remember Me with localStorage\n- Forgot Password flow with OTP 2 04/11 Register Page - Registration form: name, email, phone, password\n- User type selection: Personal/Business\n- Terms \u0026amp; Conditions checkbox 3 05/11 Authentication API - Connect to POST /api/auth/login, /register\n- JWT token management with auto-refresh\n- Protected Routes for Booking, Profile 4 06/11 User Profile Page - Display avatar, name, email, phone, address\n- Inline editing form\n- Driver License upload (required for rental) 5 07/11 Complete User Module - Booking history in User page\n- Favorite vehicles feature\n- Payment methods management\n- Notification settings Week 9 Achievements: AWS Containers:\nDeployed containers on ECS Fargate (serverless) Set up EKS cluster with managed node groups Built CI/CD pipeline for container applications Voltgo Login \u0026amp; User:\nComplete Login/Register pages with validation JWT authentication integrated User profile with avatar, driver license upload Booking history and payment methods in User page "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10: AWS Serverless \u0026amp; Voltgo Vehicle/Station Modules (10/11 - 14/11) Dual Objective: Build serverless applications on AWS and develop Vehicle/Station features for Voltgo.\nPART A: AWS - Serverless Architecture Day Date Task Activities 1 10/11 Serverless Trio - Write AWS Lambda for CRUD operations\n- Integrate with DynamoDB\n- Build REST API with API Gateway 2 11/11 Serverless Auth \u0026amp; Security - Deploy Cognito Authorizer on API Gateway\n- Configure Custom Domains and SSL 3 12/11 Event-Driven Architecture - Process orders with SQS and SNS\n- Configure S3 triggers for thumbnail generation 4 13/11 Step Functions - Build workflow orchestration\n- Create order approval process: Check inventory -\u0026gt; Charge -\u0026gt; Send email 5 14/11 X-Ray Tracing - Enable distributed tracing\n- Observe service map: API Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB PART B: Voltgo - Vehicle \u0026amp; Station Modules Day Date Task Activities 1 10/11 Vehicle List Page - Build vehicle list with grid/list toggle\n- VehicleCard: image, name, type, price, battery, status\n- Filter by type, price, range 2 11/11 Vehicle Detail Page - Image gallery, specifications display\n- Pricing table (hourly, daily, weekly)\n- Availability calendar, reviews section 3 12/11 Station List Page - Station list with StationCard component\n- Google Maps/Mapbox integration\n- Nearby stations with Geolocation API 4 13/11 Station Detail Page - Full station info: address, hours, amenities\n- Available vehicles at station\n- Charging slots status, directions 5 14/11 Map View \u0026amp; Real-time - Full-screen map with all stations/vehicles\n- Custom markers (station=green, vehicle=yellow)\n- Marker clustering, real-time updates Week 10 Achievements: AWS Serverless:\nBuilt complete serverless API with Lambda, API Gateway, DynamoDB Implemented event-driven architecture with SQS/SNS Orchestrated workflows with Step Functions Enabled distributed tracing with X-Ray Voltgo Vehicle \u0026amp; Station:\nComplete Vehicle List and Detail pages Station pages with map integration Real-time vehicle/station status updates Geolocation for nearby stations "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11: AWS Modernization \u0026amp; Voltgo Booking/Blog Module (17/11 - 21/11) Dual Goal: Modernize applications on AWS and build Booking/Blog features for Voltgo.\nPART A: AWS - Application Modernization Date Day Topic Detailed Activities 17/11 Mon Microservices Architecture - Design microservices patterns\n- Implement API composition\n- Configure service mesh basics 18/11 Tue CI/CD Pipeline - Setup CodePipeline for automated deployments\n- Configure CodeBuild for testing\n- Blue/Green deployment with CodeDeploy 19/11 Wed Container Orchestration Advanced - EKS advanced features: HPA, VPA\n- Implement service discovery\n- Configure ingress controllers 20/11 Thu Monitoring \u0026amp; Observability - CloudWatch Container Insights\n- Create custom dashboards\n- Setup alarms and notifications 21/11 Fri DevOps Best Practices - Infrastructure as Code review\n- GitOps workflow implementation\n- Security scanning in pipeline PART B: Voltgo - Booking \u0026amp; Blog Module Date Day Topic Detailed Activities 17/11 Mon Booking Flow - Step 1 - Build multi-step booking wizard\n- Step 1: Select vehicle/station\n- Step 2: Choose time slot 18/11 Tue Booking Flow - Step 2 - Step 3: User info confirmation\n- Step 4: Payment method selection\n- Price calculation with promotions 19/11 Wed Booking Confirmation - Booking confirmation page\n- Generate QR code for pickup\n- Send email/SMS notifications 20/11 Thu Blog List Page - Blog listing with categories\n- Featured/trending posts\n- Search and filter by tag 21/11 Fri Blog Detail Page - Full article view with rich content\n- Related posts sidebar\n- Social sharing, comments section Week 11 Achievements: AWS Modernization:\nDesigned microservices architecture patterns Built complete CI/CD pipeline with CodePipeline Advanced EKS configurations with auto-scaling Comprehensive monitoring with Container Insights Voltgo Booking \u0026amp; Blog:\nComplete booking flow with 4-step wizard QR code generation for vehicle pickup Blog system with categories and search Social sharing integration "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12: AWS Data Analytics \u0026amp; Voltgo Testing/Deployment (24/11 - 28/11) Dual Goal: Build data analytics solutions on AWS and prepare Voltgo for testing and deployment.\nPART A: AWS - Data Analytics \u0026amp; Machine Learning Date Day Topic Detailed Activities 24/11 Mon Data Lake Architecture - Design data lake with S3\n- Configure Lake Formation\n- Setup data catalog with Glue 25/11 Tue ETL \u0026amp; Data Processing - Build ETL jobs with Glue\n- Real-time streaming with Kinesis\n- Data transformation pipelines 26/11 Wed Analytics \u0026amp; Visualization - Query data with Athena\n- Build dashboards with QuickSight\n- Create reports and visualizations 27/11 Thu Machine Learning Basics - SageMaker overview\n- Train simple ML model\n- Deploy model endpoint 28/11 Fri AWS Well-Architected Review - Review all pillars: Security, Cost, Performance, Reliability, Operations\n- Document best practices applied PART B: Voltgo - Testing \u0026amp; Deployment Preparation Date Day Topic Detailed Activities 24/11 Mon Unit Testing - Setup Jest/React Testing Library\n- Write unit tests for components\n- Test Redux slices and actions 25/11 Tue Integration Testing - API integration tests\n- Test booking flow end-to-end\n- Mock service responses 26/11 Wed E2E Testing \u0026amp; Performance - Setup Cypress for E2E tests\n- Test critical user journeys\n- Performance audit with Lighthouse 27/11 Thu Build Optimization - Code splitting and lazy loading\n- Bundle size optimization\n- Image and asset optimization 28/11 Fri Deployment Setup - Configure CI/CD pipeline\n- Setup staging environment\n- Environment variables management Week 12 Achievements: AWS Data Analytics:\nBuilt data lake architecture with S3 and Lake Formation ETL pipelines with Glue and real-time streaming Analytics dashboards with Athena and QuickSight Introduced to SageMaker ML capabilities Voltgo Testing \u0026amp; Deployment:\nComprehensive test suite (unit, integration, E2E) Performance optimization completed CI/CD pipeline configured Ready for production deployment "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13: Final Review \u0026amp; Project Handover (01/12 - 05/12) Dual Goal: Complete AWS Cloud Journey program and finalize Voltgo project handover.\nPART A: AWS - Final Review \u0026amp; Certification Prep Date Day Topic Detailed Activities 01/12 Mon Comprehensive Review - Review all 12 weeks of AWS content\n- Identify knowledge gaps\n- Create study notes summary 02/12 Tue AWS Location Service - Integrate AWS Location Service for maps\n- Configure place indexes and geofencing\n- Implement location-based features 03/12 Wed Architecture Showcase - Present 3-tier architecture built\n- Demonstrate serverless solutions\n- Show container deployments 04/12 Thu Cost Analysis \u0026amp; Optimization - Final cost report for all resources\n- Apply optimization recommendations\n- Document cost-saving strategies 05/12 Fri Program Completion - Submit final documentation\n- Complete self-evaluation\n- Receive program completion certificate PART B: Voltgo - Production Launch \u0026amp; Handover Date Day Topic Detailed Activities 01/12 Mon Final Bug Fixes - Fix remaining bugs from QA\n- Address edge cases\n- Verify all features working 02/12 Tue Find Nearby Stations Feature - Implement station search near current location\n- Integrate AWS Location Service for map display\n- Add geolocation API for user position\n- Calculate distances and show nearest stations 02/12 Tue Production Deployment - Deploy to production environment\n- Configure production domains\n- SSL and security setup 03/12 Wed Documentation - Write technical documentation\n- Create user guide\n- Document API endpoints 04/12 Thu Knowledge Transfer - Conduct handover sessions\n- Walk through codebase with team\n- Explain architecture decisions 05/12 Fri Project Completion - Final demo to stakeholders\n- Submit project deliverables\n- Close out project tasks Week 13 Achievements: AWS Cloud Journey:\nCompleted comprehensive 13-week AWS learning program Built multiple real-world architectures Documented best practices and lessons learned Voltgo Frontend Project:\nSuccessfully deployed to production All modules completed: User, Login, Vehicle, Station, Booking, Blog Comprehensive documentation delivered Knowledge transfer completed PROGRAM COMPLETION SUMMARY AWS Cloud Journey Accomplishments: AWS Account Setup \u0026amp; IAM Management VPC \u0026amp; Networking Architecture EC2 \u0026amp; Auto Scaling S3 Storage Solutions RDS \u0026amp; Database Management Security Architecture (WAF, KMS, Secrets Manager, GuardDuty) Container Services (ECS, EKS) Serverless (Lambda, API Gateway, DynamoDB) AWS Location Service (Maps, Place Indexes, Geofencing) CI/CD \u0026amp; DevOps Practices Data Analytics \u0026amp; Machine Learning Basics Voltgo Frontend Accomplishments: Project Setup with React.js, TypeScript, Redux Toolkit Design System \u0026amp; UI Components User Management Module Authentication (Login/Register) Vehicle Module (List, Detail, Filters) Station Module (List, Detail, Map Integration) Find Nearby Stations Feature (AWS Location Service integration) Booking Flow (Multi-step wizard, QR code) Blog System (List, Detail, Categories) Comprehensive Testing Suite Production Deployment "},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://truongnguyenthaibinh77.github.io/Internship-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]